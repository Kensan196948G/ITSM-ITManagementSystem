"""
å†…éƒ¨æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
ä¿®å¾©å¾Œã®APIæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æ¤œè¨¼ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£çŠ¶æ…‹ç¢ºèªãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™æ¤œè¨¼
"""

import asyncio
import aiohttp
import aiofiles
import logging
import time
import json
import traceback
import re
import sqlite3
import subprocess
import os
import hashlib
import psutil
import ssl
import socket
import statistics
from typing import Dict, List, Optional, Any, Tuple, Union, Set
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from urllib.parse import urlparse
import threading
from concurrent.futures import ThreadPoolExecutor
from collections import defaultdict, deque
import uuid

logger = logging.getLogger(__name__)


class ValidationSeverity(Enum):
    """æ¤œè¨¼é‡è¦åº¦"""

    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


class ValidationCategory(Enum):
    """æ¤œè¨¼ã‚«ãƒ†ã‚´ãƒª"""

    API_FUNCTIONALITY = "api_functionality"
    DATABASE_INTEGRITY = "database_integrity"
    SECURITY_COMPLIANCE = "security_compliance"
    PERFORMANCE_METRICS = "performance_metrics"
    SYSTEM_HEALTH = "system_health"
    DATA_CONSISTENCY = "data_consistency"
    ENDPOINT_AVAILABILITY = "endpoint_availability"
    RESPONSE_VALIDATION = "response_validation"


@dataclass
class ValidationTest:
    """æ¤œè¨¼ãƒ†ã‚¹ãƒˆå®šç¾©"""

    test_id: str
    name: str
    category: ValidationCategory
    severity: ValidationSeverity
    description: str
    test_function: str
    timeout: int
    dependencies: List[str]
    expected_result: Any
    retry_count: int = 3


@dataclass
class ValidationResult:
    """æ¤œè¨¼çµæœ"""

    test_id: str
    test_name: str
    category: ValidationCategory
    severity: ValidationSeverity
    timestamp: datetime
    duration: float
    success: bool
    score: float  # 0-100
    actual_result: Any
    expected_result: Any
    error_message: Optional[str]
    recommendations: List[str]
    details: Dict[str, Any]


@dataclass
class ValidationSuite:
    """æ¤œè¨¼ã‚¹ã‚¤ãƒ¼ãƒˆ"""

    suite_id: str
    name: str
    description: str
    tests: List[ValidationTest]
    parallel_execution: bool
    timeout: int


class InternalValidationSystem:
    """å†…éƒ¨æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, base_url: str = "http://192.168.3.135:8000"):
        self.base_url = base_url
        self.backend_path = Path(
            "/media/kensan/LinuxHDD/ITSM-ITmanagementSystem/backend"
        )
        self.coordination_path = Path(
            "/media/kensan/LinuxHDD/ITSM-ITmanagementSystem/coordination"
        )

        # æ¤œè¨¼çµæœå±¥æ­´
        self.validation_results: deque = deque(maxlen=10000)
        self.validation_history: List[Dict[str, Any]] = []

        # æ¤œè¨¼ã‚¹ã‚¤ãƒ¼ãƒˆå®šç¾©
        self.validation_suites = self._initialize_validation_suites()

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–å€¤
        self.performance_thresholds = {
            "response_time_threshold": 3.0,  # ç§’
            "cpu_threshold": 80.0,  # %
            "memory_threshold": 85.0,  # %
            "database_query_threshold": 1.0,  # ç§’
            "error_rate_threshold": 5.0,  # %
            "availability_threshold": 99.0,  # %
        }

        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åŸºæº–
        self.security_standards = {
            "ssl_min_version": "TLSv1.2",
            "password_min_length": 8,
            "session_timeout": 3600,  # ç§’
            "max_login_attempts": 5,
            "required_headers": [
                "X-Content-Type-Options",
                "X-Frame-Options",
                "X-XSS-Protection",
            ],
        }

    def _initialize_validation_suites(self) -> Dict[str, ValidationSuite]:
        """æ¤œè¨¼ã‚¹ã‚¤ãƒ¼ãƒˆåˆæœŸåŒ–"""
        suites = {}

        # åŸºæœ¬APIæ©Ÿèƒ½æ¤œè¨¼ã‚¹ã‚¤ãƒ¼ãƒˆ
        api_tests = [
            ValidationTest(
                test_id="api_health_check",
                name="API ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯",
                category=ValidationCategory.API_FUNCTIONALITY,
                severity=ValidationSeverity.CRITICAL,
                description="APIã®åŸºæœ¬çš„ãªå¿œç­”æ€§ã‚’ç¢ºèª",
                test_function="_test_api_health",
                timeout=30,
                dependencies=[],
                expected_result={"status": "ok"},
            ),
            ValidationTest(
                test_id="api_endpoints_availability",
                name="ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå¯ç”¨æ€§",
                category=ValidationCategory.ENDPOINT_AVAILABILITY,
                severity=ValidationSeverity.ERROR,
                description="é‡è¦ãªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®å¯ç”¨æ€§ç¢ºèª",
                test_function="_test_endpoints_availability",
                timeout=60,
                dependencies=["api_health_check"],
                expected_result={"all_available": True},
            ),
            ValidationTest(
                test_id="api_response_validation",
                name="APIãƒ¬ã‚¹ãƒãƒ³ã‚¹æ¤œè¨¼",
                category=ValidationCategory.RESPONSE_VALIDATION,
                severity=ValidationSeverity.WARNING,
                description="APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æ§‹é€ ã¨å†…å®¹ã‚’æ¤œè¨¼",
                test_function="_test_api_response_validation",
                timeout=45,
                dependencies=["api_endpoints_availability"],
                expected_result={"valid_responses": True},
            ),
        ]

        suites["api_functionality"] = ValidationSuite(
            suite_id="api_functionality",
            name="APIæ©Ÿèƒ½æ¤œè¨¼",
            description="APIã®åŸºæœ¬æ©Ÿèƒ½ã¨å¿œç­”æ€§ã‚’æ¤œè¨¼",
            tests=api_tests,
            parallel_execution=False,
            timeout=180,
        )

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•´åˆæ€§æ¤œè¨¼ã‚¹ã‚¤ãƒ¼ãƒˆ
        db_tests = [
            ValidationTest(
                test_id="database_connection",
                name="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š",
                category=ValidationCategory.DATABASE_INTEGRITY,
                severity=ValidationSeverity.CRITICAL,
                description="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®æ¥ç¶šæ€§ã‚’ç¢ºèª",
                test_function="_test_database_connection",
                timeout=15,
                dependencies=[],
                expected_result={"connected": True},
            ),
            ValidationTest(
                test_id="database_integrity",
                name="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•´åˆæ€§",
                category=ValidationCategory.DATABASE_INTEGRITY,
                severity=ValidationSeverity.CRITICAL,
                description="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯",
                test_function="_test_database_integrity",
                timeout=30,
                dependencies=["database_connection"],
                expected_result={"integrity": "ok"},
            ),
            ValidationTest(
                test_id="data_consistency",
                name="ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§",
                category=ValidationCategory.DATA_CONSISTENCY,
                severity=ValidationSeverity.ERROR,
                description="ãƒ‡ãƒ¼ã‚¿ã®ä¸€è²«æ€§ã‚’æ¤œè¨¼",
                test_function="_test_data_consistency",
                timeout=60,
                dependencies=["database_integrity"],
                expected_result={"consistent": True},
            ),
        ]

        suites["database_integrity"] = ValidationSuite(
            suite_id="database_integrity",
            name="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•´åˆæ€§æ¤œè¨¼",
            description="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã‚’æ¤œè¨¼",
            tests=db_tests,
            parallel_execution=False,
            timeout=120,
        )

        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ¤œè¨¼ã‚¹ã‚¤ãƒ¼ãƒˆ
        security_tests = [
            ValidationTest(
                test_id="security_headers",
                name="ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ˜ãƒƒãƒ€ãƒ¼",
                category=ValidationCategory.SECURITY_COMPLIANCE,
                severity=ValidationSeverity.WARNING,
                description="å¿…è¦ãªã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ˜ãƒƒãƒ€ãƒ¼ã®å­˜åœ¨ç¢ºèª",
                test_function="_test_security_headers",
                timeout=30,
                dependencies=[],
                expected_result={"headers_present": True},
            ),
            ValidationTest(
                test_id="ssl_configuration",
                name="SSL/TLSè¨­å®š",
                category=ValidationCategory.SECURITY_COMPLIANCE,
                severity=ValidationSeverity.ERROR,
                description="SSL/TLSè¨­å®šã®é©åˆ‡æ€§ã‚’ç¢ºèª",
                test_function="_test_ssl_configuration",
                timeout=20,
                dependencies=[],
                expected_result={"ssl_valid": True},
            ),
            ValidationTest(
                test_id="authentication_security",
                name="èªè¨¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£",
                category=ValidationCategory.SECURITY_COMPLIANCE,
                severity=ValidationSeverity.CRITICAL,
                description="èªè¨¼æ©Ÿèƒ½ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®šç¢ºèª",
                test_function="_test_authentication_security",
                timeout=45,
                dependencies=["security_headers"],
                expected_result={"auth_secure": True},
            ),
        ]

        suites["security_compliance"] = ValidationSuite(
            suite_id="security_compliance",
            name="ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æº–æ‹ æ¤œè¨¼",
            description="ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®šã¨æº–æ‹ æ€§ã‚’æ¤œè¨¼",
            tests=security_tests,
            parallel_execution=True,
            timeout=120,
        )

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼ã‚¹ã‚¤ãƒ¼ãƒˆ
        performance_tests = [
            ValidationTest(
                test_id="response_time_performance",
                name="ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“",
                category=ValidationCategory.PERFORMANCE_METRICS,
                severity=ValidationSeverity.WARNING,
                description="APIãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®æ€§èƒ½ç¢ºèª",
                test_function="_test_response_time_performance",
                timeout=60,
                dependencies=[],
                expected_result={"avg_response_time": "< 3.0s"},
            ),
            ValidationTest(
                test_id="system_resource_performance",
                name="ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹",
                category=ValidationCategory.PERFORMANCE_METRICS,
                severity=ValidationSeverity.WARNING,
                description="CPUãƒ»ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã®ç¢ºèª",
                test_function="_test_system_resource_performance",
                timeout=30,
                dependencies=[],
                expected_result={"resource_usage": "normal"},
            ),
            ValidationTest(
                test_id="database_performance",
                name="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ€§èƒ½",
                category=ValidationCategory.PERFORMANCE_METRICS,
                severity=ValidationSeverity.WARNING,
                description="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªæ€§èƒ½ã®ç¢ºèª",
                test_function="_test_database_performance",
                timeout=45,
                dependencies=[],
                expected_result={"query_performance": "good"},
            ),
        ]

        suites["performance_metrics"] = ValidationSuite(
            suite_id="performance_metrics",
            name="ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼",
            description="ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¤œè¨¼",
            tests=performance_tests,
            parallel_execution=True,
            timeout=150,
        )

        return suites

    async def run_comprehensive_validation(
        self, suite_names: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„æ¤œè¨¼å®Ÿè¡Œ"""
        validation_id = str(uuid.uuid4())
        start_time = datetime.now()

        logger.info(f"ğŸ” åŒ…æ‹¬çš„æ¤œè¨¼é–‹å§‹ (ID: {validation_id})")

        # å®Ÿè¡Œå¯¾è±¡ã‚¹ã‚¤ãƒ¼ãƒˆã®æ±ºå®š
        if suite_names is None:
            suite_names = list(self.validation_suites.keys())

        # æ¤œè¨¼çµæœæ ¼ç´
        validation_session = {
            "validation_id": validation_id,
            "start_time": start_time.isoformat(),
            "suite_names": suite_names,
            "results": {},
            "overall_score": 0.0,
            "success_rate": 0.0,
            "execution_time": 0.0,
            "recommendations": [],
        }

        try:
            # å„ã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œ
            for suite_name in suite_names:
                if suite_name in self.validation_suites:
                    suite_result = await self._execute_validation_suite(
                        self.validation_suites[suite_name]
                    )
                    validation_session["results"][suite_name] = suite_result
                else:
                    logger.warning(f"æ¤œè¨¼ã‚¹ã‚¤ãƒ¼ãƒˆ '{suite_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

            # å…¨ä½“ã‚¹ã‚³ã‚¢è¨ˆç®—
            validation_session["overall_score"] = self._calculate_overall_score(
                validation_session["results"]
            )

            # æˆåŠŸç‡è¨ˆç®—
            validation_session["success_rate"] = self._calculate_success_rate(
                validation_session["results"]
            )

            # æ¨å¥¨äº‹é …ç”Ÿæˆ
            validation_session["recommendations"] = (
                self._generate_validation_recommendations(validation_session["results"])
            )

        except Exception as e:
            logger.error(f"åŒ…æ‹¬çš„æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            validation_session["error"] = str(e)

        finally:
            end_time = datetime.now()
            validation_session["end_time"] = end_time.isoformat()
            validation_session["execution_time"] = (
                end_time - start_time
            ).total_seconds()

            # çµæœä¿å­˜
            await self._save_validation_session(validation_session)

            logger.info(
                f"âœ… åŒ…æ‹¬çš„æ¤œè¨¼å®Œäº† (ã‚¹ã‚³ã‚¢: {validation_session['overall_score']:.1f}/100)"
            )

        return validation_session

    async def _execute_validation_suite(self, suite: ValidationSuite) -> Dict[str, Any]:
        """æ¤œè¨¼ã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ"""
        start_time = datetime.now()
        suite_result = {
            "suite_id": suite.suite_id,
            "suite_name": suite.name,
            "start_time": start_time.isoformat(),
            "test_results": [],
            "summary": {
                "total_tests": len(suite.tests),
                "passed": 0,
                "failed": 0,
                "warnings": 0,
                "errors": 0,
                "overall_score": 0.0,
            },
        }

        try:
            if suite.parallel_execution:
                # ä¸¦åˆ—å®Ÿè¡Œ
                tasks = []
                for test in suite.tests:
                    task = asyncio.create_task(self._execute_validation_test(test))
                    tasks.append(task)

                test_results = await asyncio.gather(*tasks, return_exceptions=True)
            else:
                # é †æ¬¡å®Ÿè¡Œï¼ˆä¾å­˜é–¢ä¿‚ã‚’è€ƒæ…®ï¼‰
                test_results = []
                executed_tests = set()

                for test in self._sort_tests_by_dependencies(suite.tests):
                    # ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯
                    if all(dep in executed_tests for dep in test.dependencies):
                        result = await self._execute_validation_test(test)
                        test_results.append(result)
                        executed_tests.add(test.test_id)
                    else:
                        logger.warning(
                            f"ãƒ†ã‚¹ãƒˆ {test.test_id} ã®ä¾å­˜é–¢ä¿‚ãŒæº€ãŸã•ã‚Œã¦ã„ã¾ã›ã‚“"
                        )

            # çµæœé›†è¨ˆ
            for result in test_results:
                if isinstance(result, Exception):
                    logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {result}")
                    continue

                suite_result["test_results"].append(result)

                # çµ±è¨ˆæ›´æ–°
                if result.success:
                    suite_result["summary"]["passed"] += 1
                else:
                    suite_result["summary"]["failed"] += 1

                if result.severity == ValidationSeverity.WARNING:
                    suite_result["summary"]["warnings"] += 1
                elif result.severity == ValidationSeverity.ERROR:
                    suite_result["summary"]["errors"] += 1

            # å…¨ä½“ã‚¹ã‚³ã‚¢è¨ˆç®—
            if suite_result["test_results"]:
                total_score = sum(r.score for r in suite_result["test_results"])
                suite_result["summary"]["overall_score"] = total_score / len(
                    suite_result["test_results"]
                )

        except Exception as e:
            logger.error(f"æ¤œè¨¼ã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ ({suite.suite_id}): {e}")
            suite_result["error"] = str(e)

        finally:
            end_time = datetime.now()
            suite_result["end_time"] = end_time.isoformat()
            suite_result["execution_time"] = (end_time - start_time).total_seconds()

        return suite_result

    def _sort_tests_by_dependencies(
        self, tests: List[ValidationTest]
    ) -> List[ValidationTest]:
        """ä¾å­˜é–¢ä¿‚ã«åŸºã¥ã„ã¦ãƒ†ã‚¹ãƒˆã‚’ã‚½ãƒ¼ãƒˆ"""
        sorted_tests = []
        remaining_tests = tests.copy()
        executed_test_ids = set()

        while remaining_tests:
            progress_made = False

            for test in remaining_tests[:]:
                if all(dep in executed_test_ids for dep in test.dependencies):
                    sorted_tests.append(test)
                    executed_test_ids.add(test.test_id)
                    remaining_tests.remove(test)
                    progress_made = True

            if not progress_made:
                # å¾ªç’°ä¾å­˜é–¢ä¿‚ã®å¯èƒ½æ€§
                logger.warning(
                    "å¾ªç’°ä¾å­˜é–¢ä¿‚ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚æ®‹ã‚Šã®ãƒ†ã‚¹ãƒˆã‚’ä¾å­˜é–¢ä¿‚ãªã—ã§å®Ÿè¡Œã—ã¾ã™ã€‚"
                )
                sorted_tests.extend(remaining_tests)
                break

        return sorted_tests

    async def _execute_validation_test(self, test: ValidationTest) -> ValidationResult:
        """å€‹åˆ¥æ¤œè¨¼ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        start_time = datetime.now()

        # çµæœã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåˆæœŸåŒ–
        result = ValidationResult(
            test_id=test.test_id,
            test_name=test.name,
            category=test.category,
            severity=test.severity,
            timestamp=start_time,
            duration=0.0,
            success=False,
            score=0.0,
            actual_result=None,
            expected_result=test.expected_result,
            error_message=None,
            recommendations=[],
            details={},
        )

        try:
            # ãƒ†ã‚¹ãƒˆé–¢æ•°ã‚’å–å¾—ã—ã¦å®Ÿè¡Œ
            test_function = getattr(self, test.test_function, None)
            if test_function is None:
                raise ValueError(f"ãƒ†ã‚¹ãƒˆé–¢æ•° {test.test_function} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            actual_result = await asyncio.wait_for(
                test_function(test), timeout=test.timeout
            )

            result.actual_result = actual_result

            # çµæœè©•ä¾¡
            result.success, result.score, result.recommendations = (
                self._evaluate_test_result(test, actual_result)
            )

        except asyncio.TimeoutError:
            result.error_message = f"ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({test.timeout}ç§’)"
            result.score = 0.0
            result.recommendations.append("ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œæ™‚é–“ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        except Exception as e:
            result.error_message = str(e)
            result.score = 0.0
            result.recommendations.append(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£ãŒå¿…è¦: {str(e)}")

        finally:
            end_time = datetime.now()
            result.duration = (end_time - start_time).total_seconds()

        # çµæœã‚’å±¥æ­´ã«è¿½åŠ 
        self.validation_results.append(result)

        return result

    def _evaluate_test_result(
        self, test: ValidationTest, actual_result: Any
    ) -> Tuple[bool, float, List[str]]:
        """ãƒ†ã‚¹ãƒˆçµæœè©•ä¾¡"""
        recommendations = []

        try:
            if test.category == ValidationCategory.API_FUNCTIONALITY:
                return self._evaluate_api_test(test, actual_result, recommendations)
            elif test.category == ValidationCategory.DATABASE_INTEGRITY:
                return self._evaluate_database_test(
                    test, actual_result, recommendations
                )
            elif test.category == ValidationCategory.SECURITY_COMPLIANCE:
                return self._evaluate_security_test(
                    test, actual_result, recommendations
                )
            elif test.category == ValidationCategory.PERFORMANCE_METRICS:
                return self._evaluate_performance_test(
                    test, actual_result, recommendations
                )
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè©•ä¾¡
                success = actual_result == test.expected_result
                score = 100.0 if success else 0.0
                return success, score, recommendations

        except Exception as e:
            logger.error(f"ãƒ†ã‚¹ãƒˆçµæœè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            recommendations.append(f"çµæœè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False, 0.0, recommendations

    def _evaluate_api_test(
        self, test: ValidationTest, actual_result: Any, recommendations: List[str]
    ) -> Tuple[bool, float, List[str]]:
        """API ãƒ†ã‚¹ãƒˆçµæœè©•ä¾¡"""
        if test.test_id == "api_health_check":
            if isinstance(actual_result, dict) and actual_result.get("status") == "ok":
                return True, 100.0, recommendations
            else:
                recommendations.append("APIãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ãŒå¤±æ•—ã—ã¦ã„ã¾ã™")
                return False, 0.0, recommendations

        elif test.test_id == "api_endpoints_availability":
            if isinstance(actual_result, dict):
                available_count = actual_result.get("available_count", 0)
                total_count = actual_result.get("total_count", 1)
                score = (available_count / total_count) * 100
                success = score >= 95.0

                if not success:
                    recommendations.append(
                        f"åˆ©ç”¨å¯èƒ½ãªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒä¸è¶³ã—ã¦ã„ã¾ã™ ({available_count}/{total_count})"
                    )

                return success, score, recommendations

        elif test.test_id == "api_response_validation":
            if isinstance(actual_result, dict):
                valid_responses = actual_result.get("valid_responses", 0)
                total_responses = actual_result.get("total_responses", 1)
                score = (valid_responses / total_responses) * 100
                success = score >= 90.0

                if not success:
                    recommendations.append(
                        f"APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å¦¥å½“æ€§ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ ({valid_responses}/{total_responses})"
                    )

                return success, score, recommendations

        return False, 0.0, recommendations

    def _evaluate_database_test(
        self, test: ValidationTest, actual_result: Any, recommendations: List[str]
    ) -> Tuple[bool, float, List[str]]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ ãƒ†ã‚¹ãƒˆçµæœè©•ä¾¡"""
        if test.test_id == "database_connection":
            if isinstance(actual_result, dict) and actual_result.get("connected"):
                return True, 100.0, recommendations
            else:
                recommendations.append("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã«å¤±æ•—ã—ã¦ã„ã¾ã™")
                return False, 0.0, recommendations

        elif test.test_id == "database_integrity":
            if isinstance(actual_result, dict):
                integrity = actual_result.get("integrity")
                if integrity == "ok":
                    return True, 100.0, recommendations
                else:
                    recommendations.append(
                        f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•´åˆæ€§ã«å•é¡ŒãŒã‚ã‚Šã¾ã™: {integrity}"
                    )
                    return False, 0.0, recommendations

        elif test.test_id == "data_consistency":
            if isinstance(actual_result, dict):
                consistency_score = actual_result.get("consistency_score", 0)
                success = consistency_score >= 95.0

                if not success:
                    recommendations.append(
                        f"ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ (ã‚¹ã‚³ã‚¢: {consistency_score})"
                    )

                return success, consistency_score, recommendations

        return False, 0.0, recommendations

    def _evaluate_security_test(
        self, test: ValidationTest, actual_result: Any, recommendations: List[str]
    ) -> Tuple[bool, float, List[str]]:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ ãƒ†ã‚¹ãƒˆçµæœè©•ä¾¡"""
        if test.test_id == "security_headers":
            if isinstance(actual_result, dict):
                present_headers = actual_result.get("present_headers", [])
                required_headers = self.security_standards["required_headers"]
                score = (len(present_headers) / len(required_headers)) * 100
                success = score >= 80.0

                if not success:
                    missing = set(required_headers) - set(present_headers)
                    recommendations.append(
                        f"å¿…è¦ãªã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ˜ãƒƒãƒ€ãƒ¼ãŒä¸è¶³: {list(missing)}"
                    )

                return success, score, recommendations

        elif test.test_id == "ssl_configuration":
            if isinstance(actual_result, dict):
                ssl_valid = actual_result.get("ssl_valid", False)
                if ssl_valid:
                    return True, 100.0, recommendations
                else:
                    recommendations.append("SSL/TLSè¨­å®šã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
                    return False, 0.0, recommendations

        elif test.test_id == "authentication_security":
            if isinstance(actual_result, dict):
                auth_score = actual_result.get("auth_score", 0)
                success = auth_score >= 85.0

                if not success:
                    recommendations.append(
                        f"èªè¨¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã«æ”¹å–„ãŒå¿…è¦ (ã‚¹ã‚³ã‚¢: {auth_score})"
                    )

                return success, auth_score, recommendations

        return False, 0.0, recommendations

    def _evaluate_performance_test(
        self, test: ValidationTest, actual_result: Any, recommendations: List[str]
    ) -> Tuple[bool, float, List[str]]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ ãƒ†ã‚¹ãƒˆçµæœè©•ä¾¡"""
        if test.test_id == "response_time_performance":
            if isinstance(actual_result, dict):
                avg_response_time = actual_result.get("avg_response_time", float("inf"))
                threshold = self.performance_thresholds["response_time_threshold"]

                if avg_response_time <= threshold:
                    score = 100.0
                    success = True
                elif avg_response_time <= threshold * 2:
                    score = 50.0
                    success = False
                    recommendations.append(
                        f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãŒé…ã„: {avg_response_time:.2f}ç§’"
                    )
                else:
                    score = 0.0
                    success = False
                    recommendations.append(
                        f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãŒéå¸¸ã«é…ã„: {avg_response_time:.2f}ç§’"
                    )

                return success, score, recommendations

        elif test.test_id == "system_resource_performance":
            if isinstance(actual_result, dict):
                cpu_usage = actual_result.get("cpu_usage", 0)
                memory_usage = actual_result.get("memory_usage", 0)

                cpu_score = max(0, 100 - max(0, cpu_usage - 50) * 2)
                memory_score = max(0, 100 - max(0, memory_usage - 50) * 2)
                score = (cpu_score + memory_score) / 2

                success = score >= 70.0

                if cpu_usage > self.performance_thresholds["cpu_threshold"]:
                    recommendations.append(f"CPUä½¿ç”¨ç‡ãŒé«˜ã„: {cpu_usage}%")
                if memory_usage > self.performance_thresholds["memory_threshold"]:
                    recommendations.append(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒé«˜ã„: {memory_usage}%")

                return success, score, recommendations

        elif test.test_id == "database_performance":
            if isinstance(actual_result, dict):
                query_time = actual_result.get("avg_query_time", float("inf"))
                threshold = self.performance_thresholds["database_query_threshold"]

                if query_time <= threshold:
                    score = 100.0
                    success = True
                elif query_time <= threshold * 3:
                    score = 50.0
                    success = False
                    recommendations.append(
                        f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªãŒé…ã„: {query_time:.2f}ç§’"
                    )
                else:
                    score = 0.0
                    success = False
                    recommendations.append(
                        f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªãŒéå¸¸ã«é…ã„: {query_time:.2f}ç§’"
                    )

                return success, score, recommendations

        return False, 0.0, recommendations

    # ãƒ†ã‚¹ãƒˆå®Ÿè£…é–¢æ•°ç¾¤
    async def _test_api_health(self, test: ValidationTest) -> Dict[str, Any]:
        """API ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ"""
        try:
            async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=10)
            ) as session:
                async with session.get(f"{self.base_url}/health") as response:
                    if response.status == 200:
                        data = await response.json()
                        return {"status": "ok", "response": data}
                    else:
                        return {"status": "error", "status_code": response.status}
        except Exception as e:
            return {"status": "error", "error": str(e)}

    async def _test_endpoints_availability(
        self, test: ValidationTest
    ) -> Dict[str, Any]:
        """ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå¯ç”¨æ€§ãƒ†ã‚¹ãƒˆ"""
        endpoints = [
            "/health",
            "/docs",
            "/api/v1/incidents",
            "/api/v1/users",
            "/api/v1/dashboard/metrics",
            "/api/v1/problems",
            "/api/v1/changes",
        ]

        available_count = 0
        endpoint_results = {}

        async with aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30)
        ) as session:
            for endpoint in endpoints:
                try:
                    async with session.get(f"{self.base_url}{endpoint}") as response:
                        if response.status < 500:  # 500ç•ªå°ä»¥å¤–ã¯åˆ©ç”¨å¯èƒ½ã¨ã¿ãªã™
                            available_count += 1
                            endpoint_results[endpoint] = {
                                "status": "available",
                                "code": response.status,
                            }
                        else:
                            endpoint_results[endpoint] = {
                                "status": "error",
                                "code": response.status,
                            }
                except Exception as e:
                    endpoint_results[endpoint] = {"status": "error", "error": str(e)}

        return {
            "available_count": available_count,
            "total_count": len(endpoints),
            "availability_rate": (available_count / len(endpoints)) * 100,
            "endpoint_results": endpoint_results,
        }

    async def _test_api_response_validation(
        self, test: ValidationTest
    ) -> Dict[str, Any]:
        """APIãƒ¬ã‚¹ãƒãƒ³ã‚¹æ¤œè¨¼ãƒ†ã‚¹ãƒˆ"""
        test_endpoints = [
            {"endpoint": "/health", "method": "GET", "expected_fields": ["status"]},
            {
                "endpoint": "/api/v1/incidents",
                "method": "GET",
                "expected_fields": ["items", "total"],
            },
            {
                "endpoint": "/api/v1/users",
                "method": "GET",
                "expected_fields": ["items", "total"],
            },
        ]

        valid_responses = 0
        response_results = {}

        async with aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=45)
        ) as session:
            for test_ep in test_endpoints:
                try:
                    async with session.request(
                        test_ep["method"], f"{self.base_url}{test_ep['endpoint']}"
                    ) as response:
                        if response.status < 400:
                            try:
                                data = await response.json()

                                # æœŸå¾…ã•ã‚Œã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å­˜åœ¨ç¢ºèª
                                has_expected_fields = all(
                                    field in data
                                    for field in test_ep["expected_fields"]
                                )

                                if has_expected_fields:
                                    valid_responses += 1
                                    response_results[test_ep["endpoint"]] = {
                                        "status": "valid"
                                    }
                                else:
                                    response_results[test_ep["endpoint"]] = {
                                        "status": "invalid",
                                        "reason": "missing_expected_fields",
                                        "missing": [
                                            f
                                            for f in test_ep["expected_fields"]
                                            if f not in data
                                        ],
                                    }
                            except json.JSONDecodeError:
                                response_results[test_ep["endpoint"]] = {
                                    "status": "invalid",
                                    "reason": "invalid_json",
                                }
                        else:
                            response_results[test_ep["endpoint"]] = {
                                "status": "error",
                                "status_code": response.status,
                            }
                except Exception as e:
                    response_results[test_ep["endpoint"]] = {
                        "status": "error",
                        "error": str(e),
                    }

        return {
            "valid_responses": valid_responses,
            "total_responses": len(test_endpoints),
            "validation_rate": (valid_responses / len(test_endpoints)) * 100,
            "response_results": response_results,
        }

    async def _test_database_connection(self, test: ValidationTest) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ†ã‚¹ãƒˆ"""
        try:
            db_path = self.backend_path / "itsm.db"

            if not db_path.exists():
                return {"connected": False, "reason": "database_file_not_found"}

            conn = sqlite3.connect(str(db_path), timeout=5.0)

            # ç°¡å˜ãªã‚¯ã‚¨ãƒªã§æ¥ç¶šç¢ºèª
            cursor = conn.execute("SELECT 1")
            result = cursor.fetchone()
            conn.close()

            if result and result[0] == 1:
                return {"connected": True, "database_path": str(db_path)}
            else:
                return {"connected": False, "reason": "query_failed"}

        except Exception as e:
            return {"connected": False, "reason": "connection_error", "error": str(e)}

    async def _test_database_integrity(self, test: ValidationTest) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ•´åˆæ€§ãƒ†ã‚¹ãƒˆ"""
        try:
            db_path = self.backend_path / "itsm.db"
            conn = sqlite3.connect(str(db_path), timeout=10.0)

            # æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
            cursor = conn.execute("PRAGMA integrity_check")
            integrity_result = cursor.fetchone()

            # å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ãƒã‚§ãƒƒã‚¯
            cursor = conn.execute("PRAGMA foreign_key_check")
            fk_errors = cursor.fetchall()

            conn.close()

            integrity_status = integrity_result[0] if integrity_result else "unknown"
            has_fk_errors = len(fk_errors) > 0

            return {
                "integrity": integrity_status,
                "foreign_key_errors": len(fk_errors),
                "has_errors": has_fk_errors or integrity_status != "ok",
                "details": {
                    "integrity_check": integrity_status,
                    "fk_error_count": len(fk_errors),
                },
            }

        except Exception as e:
            return {"integrity": "error", "error": str(e)}

    async def _test_data_consistency(self, test: ValidationTest) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ"""
        try:
            db_path = self.backend_path / "itsm.db"
            conn = sqlite3.connect(str(db_path), timeout=15.0)

            consistency_checks = []
            total_score = 0

            # ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ç¢ºèª
            required_tables = ["incidents", "users", "problems", "changes"]
            cursor = conn.execute(
                "SELECT name FROM sqlite_master WHERE type='table' AND name IN ({})".format(
                    ",".join(["?" for _ in required_tables])
                ),
                required_tables,
            )
            existing_tables = [row[0] for row in cursor.fetchall()]

            table_check = {
                "required_tables": required_tables,
                "existing_tables": existing_tables,
                "missing_tables": list(set(required_tables) - set(existing_tables)),
                "score": (len(existing_tables) / len(required_tables)) * 100,
            }
            consistency_checks.append(table_check)
            total_score += table_check["score"]

            # ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
            data_counts = {}
            for table in existing_tables:
                try:
                    cursor = conn.execute(f"SELECT COUNT(*) FROM {table}")
                    count = cursor.fetchone()[0]
                    data_counts[table] = count
                except Exception as e:
                    data_counts[table] = f"error: {str(e)}"

            # å‚ç…§æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆåŸºæœ¬çš„ãªã‚‚ã®ï¼‰
            ref_integrity_score = 100.0  # åŸºæœ¬ã‚¹ã‚³ã‚¢
            ref_integrity_issues = []

            # incidents ãƒ†ãƒ¼ãƒ–ãƒ«ã® assigned_to ãŒ users ãƒ†ãƒ¼ãƒ–ãƒ«ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if "incidents" in existing_tables and "users" in existing_tables:
                try:
                    cursor = conn.execute(
                        """
                        SELECT COUNT(*) FROM incidents 
                        WHERE assigned_to IS NOT NULL 
                        AND assigned_to NOT IN (SELECT id FROM users)
                    """
                    )
                    orphaned_assignments = cursor.fetchone()[0]
                    if orphaned_assignments > 0:
                        ref_integrity_issues.append(
                            f"Orphaned incident assignments: {orphaned_assignments}"
                        )
                        ref_integrity_score -= 20
                except Exception as e:
                    ref_integrity_issues.append(f"Assignment check error: {str(e)}")
                    ref_integrity_score -= 10

            consistency_checks.append(
                {
                    "check_type": "referential_integrity",
                    "score": max(0, ref_integrity_score),
                    "issues": ref_integrity_issues,
                }
            )
            total_score += max(0, ref_integrity_score)

            conn.close()

            # å¹³å‡ã‚¹ã‚³ã‚¢è¨ˆç®—
            avg_consistency_score = (
                total_score / len(consistency_checks) if consistency_checks else 0
            )

            return {
                "consistent": avg_consistency_score >= 95.0,
                "consistency_score": avg_consistency_score,
                "checks": consistency_checks,
                "data_counts": data_counts,
                "summary": {
                    "total_checks": len(consistency_checks),
                    "avg_score": avg_consistency_score,
                },
            }

        except Exception as e:
            return {"consistent": False, "error": str(e)}

    async def _test_security_headers(self, test: ValidationTest) -> Dict[str, Any]:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆ"""
        try:
            async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=20)
            ) as session:
                async with session.get(f"{self.base_url}/health") as response:
                    headers = dict(response.headers)

                    required_headers = self.security_standards["required_headers"]
                    present_headers = []

                    for header in required_headers:
                        if header.lower() in [h.lower() for h in headers.keys()]:
                            present_headers.append(header)

                    return {
                        "headers_present": len(present_headers)
                        == len(required_headers),
                        "present_headers": present_headers,
                        "missing_headers": list(
                            set(required_headers) - set(present_headers)
                        ),
                        "all_headers": headers,
                        "compliance_rate": (
                            len(present_headers) / len(required_headers)
                        )
                        * 100,
                    }

        except Exception as e:
            return {"headers_present": False, "error": str(e)}

    async def _test_ssl_configuration(self, test: ValidationTest) -> Dict[str, Any]:
        """SSL/TLSè¨­å®šãƒ†ã‚¹ãƒˆ"""
        try:
            # HTTPSã§ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if not self.base_url.startswith("https://"):
                return {
                    "ssl_valid": True,
                    "note": "HTTPæ¥ç¶šã®ãŸã‚SSLãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—",
                    "protocol": "HTTP",
                }

            # HTTPSã®å ´åˆã®SSLç¢ºèª
            parsed_url = urlparse(self.base_url)
            hostname = parsed_url.hostname
            port = parsed_url.port or 443

            context = ssl.create_default_context()

            with socket.create_connection((hostname, port), timeout=10) as sock:
                with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                    cert = ssock.getpeercert()
                    protocol = ssock.version()

                    # è¨¼æ˜æ›¸æœ‰åŠ¹æœŸé™ãƒã‚§ãƒƒã‚¯
                    expiry_date = datetime.strptime(
                        cert["notAfter"], "%b %d %H:%M:%S %Y %Z"
                    )
                    days_until_expiry = (expiry_date - datetime.now()).days

                    return {
                        "ssl_valid": True,
                        "protocol": protocol,
                        "certificate": {
                            "subject": cert.get("subject", []),
                            "issuer": cert.get("issuer", []),
                            "expires": cert.get("notAfter"),
                            "days_until_expiry": days_until_expiry,
                        },
                        "secure": days_until_expiry > 30
                        and protocol in ["TLSv1.2", "TLSv1.3"],
                    }

        except Exception as e:
            return {"ssl_valid": False, "error": str(e)}

    async def _test_authentication_security(
        self, test: ValidationTest
    ) -> Dict[str, Any]:
        """èªè¨¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ"""
        try:
            auth_score = 100.0
            security_issues = []

            # ãƒ­ã‚°ã‚¤ãƒ³ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®å­˜åœ¨ç¢ºèª
            async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=30)
            ) as session:
                # ãƒ­ã‚°ã‚¤ãƒ³ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãƒ†ã‚¹ãƒˆ
                try:
                    async with session.post(
                        f"{self.base_url}/api/v1/auth/login"
                    ) as response:
                        if response.status in [400, 422]:  # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼ã¯æ­£å¸¸
                            pass
                        elif response.status == 404:
                            security_issues.append(
                                "ãƒ­ã‚°ã‚¤ãƒ³ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
                            )
                            auth_score -= 30
                        elif response.status >= 500:
                            security_issues.append(
                                "ãƒ­ã‚°ã‚¤ãƒ³ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼"
                            )
                            auth_score -= 20
                except Exception as e:
                    security_issues.append(
                        f"ãƒ­ã‚°ã‚¤ãƒ³ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}"
                    )
                    auth_score -= 25

                # ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒãƒªã‚·ãƒ¼ã®ç¢ºèªï¼ˆé–“æ¥çš„ï¼‰
                # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰è¦ä»¶ã‚’ç¢ºèªã™ã‚‹APIãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯

                # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã®ç¢ºèª
                # ã‚¯ãƒƒã‚­ãƒ¼ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®šãªã©

            return {
                "auth_secure": auth_score >= 85.0,
                "auth_score": auth_score,
                "security_issues": security_issues,
                "checks": {
                    "login_endpoint": "available" if auth_score >= 70 else "issues",
                    "password_policy": "not_tested",  # å®Ÿè£…ã«ä¾å­˜
                    "session_management": "not_tested",  # å®Ÿè£…ã«ä¾å­˜
                },
            }

        except Exception as e:
            return {"auth_secure": False, "error": str(e)}

    async def _test_response_time_performance(
        self, test: ValidationTest
    ) -> Dict[str, Any]:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        try:
            endpoints = ["/health", "/api/v1/incidents", "/api/v1/users"]
            response_times = []
            endpoint_times = {}

            async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=45)
            ) as session:
                for endpoint in endpoints:
                    endpoint_response_times = []

                    # å„ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’3å›ãƒ†ã‚¹ãƒˆ
                    for _ in range(3):
                        start_time = time.time()
                        try:
                            async with session.get(
                                f"{self.base_url}{endpoint}"
                            ) as response:
                                await response.read()  # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æœ¬æ–‡ã‚’èª­ã¿è¾¼ã¿
                                response_time = time.time() - start_time
                                endpoint_response_times.append(response_time)
                                response_times.append(response_time)
                        except Exception as e:
                            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚æ™‚é–“ã‚’è¨˜éŒ²ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç­‰ã‚’å«ã‚€ï¼‰
                            response_time = time.time() - start_time
                            endpoint_response_times.append(response_time)
                            response_times.append(response_time)

                    endpoint_times[endpoint] = {
                        "avg_time": statistics.mean(endpoint_response_times),
                        "max_time": max(endpoint_response_times),
                        "min_time": min(endpoint_response_times),
                    }

            avg_response_time = (
                statistics.mean(response_times) if response_times else float("inf")
            )
            max_response_time = max(response_times) if response_times else float("inf")

            return {
                "avg_response_time": avg_response_time,
                "max_response_time": max_response_time,
                "endpoint_details": endpoint_times,
                "threshold": self.performance_thresholds["response_time_threshold"],
                "performance_rating": (
                    "good"
                    if avg_response_time
                    <= self.performance_thresholds["response_time_threshold"]
                    else "poor"
                ),
            }

        except Exception as e:
            return {"avg_response_time": float("inf"), "error": str(e)}

    async def _test_system_resource_performance(
        self, test: ValidationTest
    ) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        try:
            # è¤‡æ•°å›æ¸¬å®šã—ã¦å¹³å‡ã‚’å–ã‚‹
            cpu_measurements = []
            memory_measurements = []

            for _ in range(5):
                cpu_measurements.append(psutil.cpu_percent(interval=0.2))
                memory_measurements.append(psutil.virtual_memory().percent)
                await asyncio.sleep(0.1)

            cpu_usage = statistics.mean(cpu_measurements)
            memory_usage = statistics.mean(memory_measurements)

            # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡
            disk_usage = psutil.disk_usage("/").percent

            # ãƒ—ãƒ­ã‚»ã‚¹æ•°
            process_count = len(psutil.pids())

            return {
                "cpu_usage": cpu_usage,
                "memory_usage": memory_usage,
                "disk_usage": disk_usage,
                "process_count": process_count,
                "thresholds": {
                    "cpu_threshold": self.performance_thresholds["cpu_threshold"],
                    "memory_threshold": self.performance_thresholds["memory_threshold"],
                },
                "status": {
                    "cpu": (
                        "normal"
                        if cpu_usage < self.performance_thresholds["cpu_threshold"]
                        else "high"
                    ),
                    "memory": (
                        "normal"
                        if memory_usage
                        < self.performance_thresholds["memory_threshold"]
                        else "high"
                    ),
                },
            }

        except Exception as e:
            return {"cpu_usage": 0, "memory_usage": 0, "error": str(e)}

    async def _test_database_performance(self, test: ValidationTest) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        try:
            db_path = self.backend_path / "itsm.db"
            conn = sqlite3.connect(str(db_path), timeout=15.0)

            query_times = []
            query_results = {}

            # åŸºæœ¬çš„ãªã‚¯ã‚¨ãƒªã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
            test_queries = [
                ("simple_select", "SELECT 1"),
                ("count_incidents", "SELECT COUNT(*) FROM incidents"),
                ("count_users", "SELECT COUNT(*) FROM users"),
            ]

            for query_name, query in test_queries:
                try:
                    start_time = time.time()
                    cursor = conn.execute(query)
                    result = cursor.fetchone()
                    query_time = time.time() - start_time

                    query_times.append(query_time)
                    query_results[query_name] = {
                        "time": query_time,
                        "result": result,
                        "status": "success",
                    }
                except Exception as e:
                    query_results[query_name] = {
                        "time": 0,
                        "error": str(e),
                        "status": "error",
                    }

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚ºç¢ºèª
            db_size_mb = db_path.stat().st_size / (1024 * 1024)

            conn.close()

            avg_query_time = (
                statistics.mean(query_times) if query_times else float("inf")
            )
            max_query_time = max(query_times) if query_times else float("inf")

            return {
                "avg_query_time": avg_query_time,
                "max_query_time": max_query_time,
                "database_size_mb": db_size_mb,
                "query_details": query_results,
                "threshold": self.performance_thresholds["database_query_threshold"],
                "performance_rating": (
                    "good"
                    if avg_query_time
                    <= self.performance_thresholds["database_query_threshold"]
                    else "poor"
                ),
            }

        except Exception as e:
            return {"avg_query_time": float("inf"), "error": str(e)}

    def _calculate_overall_score(self, results: Dict[str, Any]) -> float:
        """å…¨ä½“ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        total_score = 0.0
        total_weight = 0.0

        # ã‚¹ã‚¤ãƒ¼ãƒˆé‡ã¿ä»˜ã‘
        suite_weights = {
            "api_functionality": 1.0,
            "database_integrity": 1.2,
            "security_compliance": 1.1,
            "performance_metrics": 0.8,
        }

        for suite_name, suite_result in results.items():
            if "summary" in suite_result and "overall_score" in suite_result["summary"]:
                weight = suite_weights.get(suite_name, 1.0)
                score = suite_result["summary"]["overall_score"]
                total_score += score * weight
                total_weight += weight

        return total_score / total_weight if total_weight > 0 else 0.0

    def _calculate_success_rate(self, results: Dict[str, Any]) -> float:
        """æˆåŠŸç‡è¨ˆç®—"""
        total_tests = 0
        passed_tests = 0

        for suite_result in results.values():
            if "summary" in suite_result:
                summary = suite_result["summary"]
                total_tests += summary.get("total_tests", 0)
                passed_tests += summary.get("passed", 0)

        return (passed_tests / total_tests * 100) if total_tests > 0 else 0.0

    def _generate_validation_recommendations(
        self, results: Dict[str, Any]
    ) -> List[str]:
        """æ¤œè¨¼æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []

        try:
            for suite_name, suite_result in results.items():
                if "test_results" in suite_result:
                    for test_result in suite_result["test_results"]:
                        if not test_result.success and test_result.recommendations:
                            recommendations.extend(test_result.recommendations)

                # ã‚¹ã‚¤ãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«ã®æ¨å¥¨äº‹é …
                if "summary" in suite_result:
                    summary = suite_result["summary"]

                    if summary.get("overall_score", 0) < 70:
                        recommendations.append(
                            f"{suite_name} ã‚¹ã‚¤ãƒ¼ãƒˆã®ã‚¹ã‚³ã‚¢ãŒä½ã„ ({summary['overall_score']:.1f}/100)"
                        )

                    if summary.get("failed", 0) > 0:
                        recommendations.append(
                            f"{suite_name} ã§ {summary['failed']} ä»¶ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—"
                        )

            # é‡è¤‡å‰Šé™¤
            recommendations = list(set(recommendations))

        except Exception as e:
            logger.error(f"æ¨å¥¨äº‹é …ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            recommendations.append(f"æ¨å¥¨äº‹é …ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")

        return recommendations

    async def _save_validation_session(self, session: Dict[str, Any]):
        """æ¤œè¨¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜"""
        try:
            # å±¥æ­´ã«è¿½åŠ 
            self.validation_history.append(session)

            # æœ€æ–°100ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã¿ä¿æŒ
            self.validation_history = self.validation_history[-100:]

            # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            timestamp = int(datetime.now().timestamp())
            session_file = (
                self.coordination_path / f"validation_session_{timestamp}.json"
            )

            async with aiofiles.open(session_file, "w") as f:
                await f.write(json.dumps(session, indent=2, ensure_ascii=False))

            # æœ€æ–°ã®æ¤œè¨¼çµæœã‚’ä¿å­˜
            latest_file = self.coordination_path / "latest_validation_results.json"
            async with aiofiles.open(latest_file, "w") as f:
                await f.write(json.dumps(session, indent=2, ensure_ascii=False))

            logger.info(f"ğŸ“‹ æ¤œè¨¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜å®Œäº†: {session_file}")

        except Exception as e:
            logger.error(f"æ¤œè¨¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def get_validation_status(self) -> Dict[str, Any]:
        """æ¤œè¨¼çŠ¶æ³å–å¾—"""
        try:
            latest_session = (
                self.validation_history[-1] if self.validation_history else None
            )

            return {
                "total_sessions": len(self.validation_history),
                "total_validations": len(self.validation_results),
                "latest_session": (
                    {
                        "validation_id": (
                            latest_session.get("validation_id")
                            if latest_session
                            else None
                        ),
                        "timestamp": (
                            latest_session.get("start_time") if latest_session else None
                        ),
                        "overall_score": (
                            latest_session.get("overall_score") if latest_session else 0
                        ),
                        "success_rate": (
                            latest_session.get("success_rate") if latest_session else 0
                        ),
                    }
                    if latest_session
                    else None
                ),
                "available_suites": list(self.validation_suites.keys()),
            }
        except Exception as e:
            logger.error(f"æ¤œè¨¼çŠ¶æ³å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
internal_validator = InternalValidationSystem()


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        logger.info("ğŸ” å†…éƒ¨æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ å˜ä½“å®Ÿè¡Œé–‹å§‹")

        # å…¨æ¤œè¨¼ã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œ
        result = await internal_validator.run_comprehensive_validation()

        print("\n=== æ¤œè¨¼çµæœæ¦‚è¦ ===")
        print(f"å…¨ä½“ã‚¹ã‚³ã‚¢: {result['overall_score']:.1f}/100")
        print(f"æˆåŠŸç‡: {result['success_rate']:.1f}%")
        print(f"å®Ÿè¡Œæ™‚é–“: {result['execution_time']:.2f}ç§’")

        if result["recommendations"]:
            print("\n=== æ¨å¥¨äº‹é … ===")
            for rec in result["recommendations"]:
                print(f"- {rec}")

        print(
            f"\nè©³ç´°çµæœã¯ {internal_validator.coordination_path / 'latest_validation_results.json'} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ"
        )

    except KeyboardInterrupt:
        logger.info("âŒ¨ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(
                "/media/kensan/LinuxHDD/ITSM-ITmanagementSystem/coordination/internal_validation.log"
            ),
            logging.StreamHandler(),
        ],
    )

    asyncio.run(main())
