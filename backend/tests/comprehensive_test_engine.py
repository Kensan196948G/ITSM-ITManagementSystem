#!/usr/bin/env python3
"""
ã€ãƒ•ã‚§ãƒ¼ã‚º2ã€‘ITSM CI/CD åŒ…æ‹¬çš„è‡ªå‹•ãƒ†ã‚¹ãƒˆã‚¨ãƒ³ã‚¸ãƒ³
- E2Eãƒ†ã‚¹ãƒˆã€APIãƒ†ã‚¹ãƒˆã€è² è·ãƒ†ã‚¹ãƒˆã®çµ±åˆå®Ÿè¡Œ
- ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³
- CI/CDå“è³ªã‚²ãƒ¼ãƒˆåˆ¤å®šã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""

import asyncio
import json
import time
import subprocess
import sys
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
import logging
import psutil
import aiohttp
import aiofiles
from dataclasses import dataclass, asdict


@dataclass
class TestResult:
    """ãƒ†ã‚¹ãƒˆçµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    name: str
    status: str  # PASS, FAIL, SKIP
    duration: float
    coverage: Optional[float] = None
    details: Optional[Dict] = None
    error_message: Optional[str] = None


@dataclass
class QualityGate:
    """å“è³ªã‚²ãƒ¼ãƒˆåŸºæº–"""
    min_test_coverage: float = 80.0
    max_failed_tests: int = 0
    max_security_issues: int = 0
    max_response_time: float = 2.0
    min_performance_score: float = 70.0


class ComprehensiveTestEngine:
    """åŒ…æ‹¬çš„è‡ªå‹•ãƒ†ã‚¹ãƒˆã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.backend_root = project_root / "backend"
        self.frontend_root = project_root / "frontend"
        self.test_start_time = time.time()
        
        # ãƒ­ã‚°è¨­å®š
        self.setup_logging()
        
        # ãƒ†ã‚¹ãƒˆçµæœä¿å­˜
        self.results: List[TestResult] = []
        self.quality_gate = QualityGate()
        
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œçµ±è¨ˆ
        self.stats = {
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 0,
            "skipped_tests": 0,
            "coverage": 0.0,
            "performance_score": 0.0,
            "security_issues": 0,
            "total_duration": 0.0
        }
    
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        log_dir = self.backend_root / "logs"
        log_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / "comprehensive_test.log"),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    async def run_comprehensive_tests(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        self.logger.info("ğŸš€ ãƒ•ã‚§ãƒ¼ã‚º2: åŒ…æ‹¬çš„è‡ªå‹•ãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        try:
            # 1. ç’°å¢ƒæº–å‚™ã¨ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
            await self.prepare_test_environment()
            
            # 2. å˜ä½“ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            await self.run_unit_tests()
            
            # 3. APIçµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            await self.run_api_integration_tests()
            
            # 4. E2Eãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            await self.run_e2e_tests()
            
            # 5. è² è·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            await self.run_load_tests()
            
            # 6. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            await self.run_security_tests()
            
            # 7. ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯
            await self.run_code_quality_checks()
            
            # 8. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
            await self.run_performance_tests()
            
            # 9. å“è³ªã‚²ãƒ¼ãƒˆåˆ¤å®š
            quality_result = await self.evaluate_quality_gate()
            
            # 10. åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            comprehensive_report = await self.generate_comprehensive_report()
            
            return {
                "success": quality_result["passed"],
                "summary": self.stats,
                "quality_gate": quality_result,
                "report_path": comprehensive_report["report_path"],
                "recommendations": comprehensive_report["recommendations"]
            }
            
        except Exception as e:
            self.logger.error(f"åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "success": False,
                "error": str(e),
                "summary": self.stats
            }
    
    async def prepare_test_environment(self):
        """ãƒ†ã‚¹ãƒˆç’°å¢ƒæº–å‚™"""
        self.logger.info("ğŸ”§ ãƒ†ã‚¹ãƒˆç’°å¢ƒæº–å‚™ä¸­...")
        
        # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•ç¢ºèª
        backend_health = await self.check_backend_health()
        if not backend_health:
            self.logger.warning("ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ä¸­...")
            await self.start_backend_server()
        
        # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•ç¢ºèª
        frontend_health = await self.check_frontend_health()
        if not frontend_health:
            self.logger.warning("ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ä¸­...")
            await self.start_frontend_server()
        
        # ãƒ†ã‚¹ãƒˆä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        await self.install_test_dependencies()
        
        self.logger.info("âœ… ãƒ†ã‚¹ãƒˆç’°å¢ƒæº–å‚™å®Œäº†")
    
    async def check_backend_health(self) -> bool:
        """ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get("http://localhost:8000/health") as response:
                    return response.status == 200
        except:
            return False
    
    async def check_frontend_health(self) -> bool:
        """ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get("http://localhost:3000") as response:
                    return response.status == 200
        except:
            return False
    
    async def start_backend_server(self):
        """ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒãƒ¼èµ·å‹•"""
        cmd = f"cd {self.backend_root} && python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 &"
        subprocess.Popen(cmd, shell=True)
        await asyncio.sleep(5)  # èµ·å‹•å¾…æ©Ÿ
    
    async def start_frontend_server(self):
        """ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒãƒ¼èµ·å‹•"""
        cmd = f"cd {self.frontend_root} && npm run dev &"
        subprocess.Popen(cmd, shell=True)
        await asyncio.sleep(10)  # èµ·å‹•å¾…æ©Ÿ
    
    async def install_test_dependencies(self):
        """ãƒ†ã‚¹ãƒˆä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
        # Pythonä¾å­˜é–¢ä¿‚
        cmd = f"cd {self.backend_root} && pip install -r requirements-test-enhanced.txt"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        
        if result.returncode != 0:
            self.logger.warning(f"Pythonä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«è­¦å‘Š: {result.stderr}")
        
        # Playwright ãƒ–ãƒ©ã‚¦ã‚¶ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        cmd = f"cd {self.frontend_root} && npm run playwright:install"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        
        if result.returncode != 0:
            self.logger.warning(f"Playwright ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«è­¦å‘Š: {result.stderr}")
    
    async def run_unit_tests(self):
        """å˜ä½“ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        self.logger.info("ğŸ§ª å˜ä½“ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        start_time = time.time()
        
        cmd = f"""
        cd {self.backend_root} && 
        python -m pytest tests/unit/ 
        --cov=app 
        --cov-report=html:tests/reports/coverage_html 
        --cov-report=json:tests/reports/coverage.json 
        --html=tests/reports/unit-report.html 
        --json-report --json-report-file=tests/reports/unit-report.json 
        -v
        """
        
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        duration = time.time() - start_time
        
        # ã‚«ãƒãƒ¬ãƒƒã‚¸æƒ…å ±å–å¾—
        coverage = await self.extract_coverage_info()
        
        test_result = TestResult(
            name="Unit Tests",
            status="PASS" if result.returncode == 0 else "FAIL",
            duration=duration,
            coverage=coverage,
            details={"stdout": result.stdout, "stderr": result.stderr}
        )
        
        if result.returncode != 0:
            test_result.error_message = result.stderr
        
        self.results.append(test_result)
        self.update_stats(test_result)
        
        self.logger.info(f"âœ… å˜ä½“ãƒ†ã‚¹ãƒˆå®Œäº† - {test_result.status} ({duration:.2f}s)")
    
    async def run_api_integration_tests(self):
        """APIçµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        self.logger.info("ğŸ”— APIçµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        start_time = time.time()
        
        cmd = f"""
        cd {self.backend_root} && 
        python -m pytest tests/api/ 
        --html=tests/reports/api-report.html 
        --json-report --json-report-file=tests/reports/api-report.json 
        -v
        """
        
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        duration = time.time() - start_time
        
        test_result = TestResult(
            name="API Integration Tests",
            status="PASS" if result.returncode == 0 else "FAIL",
            duration=duration,
            details={"stdout": result.stdout, "stderr": result.stderr}
        )
        
        if result.returncode != 0:
            test_result.error_message = result.stderr
        
        self.results.append(test_result)
        self.update_stats(test_result)
        
        self.logger.info(f"âœ… APIçµ±åˆãƒ†ã‚¹ãƒˆå®Œäº† - {test_result.status} ({duration:.2f}s)")
    
    async def run_e2e_tests(self):
        """E2Eãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        self.logger.info("ğŸ­ E2Eãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        start_time = time.time()
        
        cmd = f"""
        cd {self.frontend_root} && 
        npx playwright test 
        --reporter=html,json 
        --output-dir=test-results
        """
        
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        duration = time.time() - start_time
        
        test_result = TestResult(
            name="E2E Tests",
            status="PASS" if result.returncode == 0 else "FAIL",
            duration=duration,
            details={"stdout": result.stdout, "stderr": result.stderr}
        )
        
        if result.returncode != 0:
            test_result.error_message = result.stderr
        
        self.results.append(test_result)
        self.update_stats(test_result)
        
        self.logger.info(f"âœ… E2Eãƒ†ã‚¹ãƒˆå®Œäº† - {test_result.status} ({duration:.2f}s)")
    
    async def run_load_tests(self):
        """è² è·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        self.logger.info("âš¡ è² è·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        start_time = time.time()
        
        # Locustã‚’ä½¿ç”¨ã—ãŸè² è·ãƒ†ã‚¹ãƒˆ
        load_test_script = await self.create_load_test_script()
        
        cmd = f"""
        cd {self.backend_root} && 
        locust -f {load_test_script} 
        --host=http://localhost:8000 
        --users=10 
        --spawn-rate=2 
        --run-time=60s 
        --headless 
        --html=tests/reports/load-report.html 
        --csv=tests/reports/load-report
        """
        
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        duration = time.time() - start_time
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®æŠ½å‡º
        performance_score = await self.calculate_performance_score()
        
        test_result = TestResult(
            name="Load Tests",
            status="PASS" if result.returncode == 0 else "FAIL",
            duration=duration,
            details={
                "performance_score": performance_score,
                "stdout": result.stdout, 
                "stderr": result.stderr
            }
        )
        
        self.results.append(test_result)
        self.update_stats(test_result)
        
        self.logger.info(f"âœ… è² è·ãƒ†ã‚¹ãƒˆå®Œäº† - {test_result.status} ({duration:.2f}s)")
    
    async def run_security_tests(self):
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        self.logger.info("ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        start_time = time.time()
        
        # Banditã«ã‚ˆã‚‹é™çš„ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åˆ†æ
        cmd = f"""
        cd {self.backend_root} && 
        bandit -r app/ 
        -f json 
        -o tests/reports/security-report.json
        """
        
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        duration = time.time() - start_time
        
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œæ•°ã®ç®—å‡º
        security_issues = await self.count_security_issues()
        
        test_result = TestResult(
            name="Security Tests",
            status="PASS" if security_issues == 0 else "FAIL",
            duration=duration,
            details={
                "security_issues": security_issues,
                "stdout": result.stdout,
                "stderr": result.stderr
            }
        )
        
        if security_issues > 0:
            test_result.error_message = f"{security_issues} security issues found"
        
        self.results.append(test_result)
        self.update_stats(test_result)
        
        self.logger.info(f"âœ… ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆå®Œäº† - {test_result.status} ({duration:.2f}s)")
    
    async def run_code_quality_checks(self):
        """ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯"""
        self.logger.info("ğŸ“Š ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œä¸­...")
        start_time = time.time()
        
        # Flake8, Black, isort, mypyã®å®Ÿè¡Œ
        quality_checks = [
            ("flake8", f"cd {self.backend_root} && flake8 app/ --output-file=tests/reports/flake8-report.txt"),
            ("black", f"cd {self.backend_root} && black --check app/"),
            ("isort", f"cd {self.backend_root} && isort --check-only app/"),
            ("mypy", f"cd {self.backend_root} && mypy app/ --html-report tests/reports/mypy-report")
        ]
        
        all_passed = True
        check_results = {}
        
        for check_name, cmd in quality_checks:
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
            check_results[check_name] = {
                "status": "PASS" if result.returncode == 0 else "FAIL",
                "output": result.stdout + result.stderr
            }
            if result.returncode != 0:
                all_passed = False
        
        duration = time.time() - start_time
        
        test_result = TestResult(
            name="Code Quality Checks",
            status="PASS" if all_passed else "FAIL",
            duration=duration,
            details=check_results
        )
        
        self.results.append(test_result)
        self.update_stats(test_result)
        
        self.logger.info(f"âœ… ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯å®Œäº† - {test_result.status} ({duration:.2f}s)")
    
    async def run_performance_tests(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        self.logger.info("ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        start_time = time.time()
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¸¬å®š
        performance_metrics = await self.measure_performance_metrics()
        
        duration = time.time() - start_time
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢ç®—å‡º
        performance_score = await self.calculate_performance_score()
        
        test_result = TestResult(
            name="Performance Tests",
            status="PASS" if performance_score >= self.quality_gate.min_performance_score else "FAIL",
            duration=duration,
            details={
                "performance_score": performance_score,
                "metrics": performance_metrics
            }
        )
        
        self.results.append(test_result)
        self.update_stats(test_result)
        
        self.logger.info(f"âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Œäº† - {test_result.status} ({duration:.2f}s)")
    
    async def evaluate_quality_gate(self) -> Dict[str, Any]:
        """å“è³ªã‚²ãƒ¼ãƒˆè©•ä¾¡"""
        self.logger.info("ğŸšª å“è³ªã‚²ãƒ¼ãƒˆè©•ä¾¡ä¸­...")
        
        gate_results = {
            "passed": True,
            "checks": {},
            "summary": {}
        }
        
        # ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒã‚§ãƒƒã‚¯
        coverage_check = self.stats["coverage"] >= self.quality_gate.min_test_coverage
        gate_results["checks"]["coverage"] = {
            "passed": coverage_check,
            "actual": self.stats["coverage"],
            "required": self.quality_gate.min_test_coverage
        }
        
        # å¤±æ•—ãƒ†ã‚¹ãƒˆæ•°ãƒã‚§ãƒƒã‚¯
        failed_tests_check = self.stats["failed_tests"] <= self.quality_gate.max_failed_tests
        gate_results["checks"]["failed_tests"] = {
            "passed": failed_tests_check,
            "actual": self.stats["failed_tests"],
            "required": self.quality_gate.max_failed_tests
        }
        
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œãƒã‚§ãƒƒã‚¯
        security_check = self.stats["security_issues"] <= self.quality_gate.max_security_issues
        gate_results["checks"]["security"] = {
            "passed": security_check,
            "actual": self.stats["security_issues"],
            "required": self.quality_gate.max_security_issues
        }
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯
        performance_check = self.stats["performance_score"] >= self.quality_gate.min_performance_score
        gate_results["checks"]["performance"] = {
            "passed": performance_check,
            "actual": self.stats["performance_score"],
            "required": self.quality_gate.min_performance_score
        }
        
        # ç·åˆåˆ¤å®š
        gate_results["passed"] = all(
            check["passed"] for check in gate_results["checks"].values()
        )
        
        gate_results["summary"] = {
            "total_checks": len(gate_results["checks"]),
            "passed_checks": sum(1 for check in gate_results["checks"].values() if check["passed"]),
            "overall_status": "PASS" if gate_results["passed"] else "FAIL"
        }
        
        self.logger.info(f"ğŸšª å“è³ªã‚²ãƒ¼ãƒˆè©•ä¾¡å®Œäº† - {gate_results['summary']['overall_status']}")
        
        return gate_results
    
    async def generate_comprehensive_report(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        self.logger.info("ğŸ“„ åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
        
        total_duration = time.time() - self.test_start_time
        
        report_data = {
            "timestamp": datetime.now().isoformat(),
            "project": "ITSM ITmanagement System",
            "phase": "Phase 2 - Comprehensive Testing",
            "duration": total_duration,
            "summary": self.stats,
            "results": [asdict(result) for result in self.results],
            "quality_gate": await self.evaluate_quality_gate(),
            "environment": {
                "python_version": sys.version,
                "platform": os.name,
                "cpu_count": os.cpu_count(),
                "memory_gb": psutil.virtual_memory().total / (1024**3)
            }
        }
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        report_dir = self.backend_root / "tests" / "reports"
        report_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON ãƒ¬ãƒãƒ¼ãƒˆ
        json_report_path = report_dir / f"comprehensive_test_report_{timestamp}.json"
        async with aiofiles.open(json_report_path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(report_data, indent=2, ensure_ascii=False))
        
        # Markdown ãƒ¬ãƒãƒ¼ãƒˆ
        markdown_report = await self.generate_markdown_report(report_data)
        md_report_path = report_dir / f"comprehensive_test_report_{timestamp}.md"
        async with aiofiles.open(md_report_path, 'w', encoding='utf-8') as f:
            await f.write(markdown_report)
        
        # æ”¹å–„ææ¡ˆç”Ÿæˆ
        recommendations = await self.generate_recommendations()
        
        self.logger.info(f"ğŸ“„ åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {json_report_path}")
        
        return {
            "report_path": str(json_report_path),
            "markdown_path": str(md_report_path),
            "recommendations": recommendations
        }
    
    async def generate_markdown_report(self, report_data: Dict) -> str:
        """Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        md_content = f"""# ã€ãƒ•ã‚§ãƒ¼ã‚º2ã€‘ITSM CI/CD åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ

## ğŸ“Š å®Ÿè¡Œã‚µãƒãƒªãƒ¼

**å®Ÿè¡Œæ—¥æ™‚**: {report_data['timestamp']}  
**ç·å®Ÿè¡Œæ™‚é–“**: {report_data['duration']:.2f}ç§’  
**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**: {report_data['project']}  

## ğŸ¯ ãƒ†ã‚¹ãƒˆçµæœçµ±è¨ˆ

| é …ç›® | å€¤ |
|------|-----|
| ç·ãƒ†ã‚¹ãƒˆæ•° | {report_data['summary']['total_tests']} |
| æˆåŠŸãƒ†ã‚¹ãƒˆ | {report_data['summary']['passed_tests']} |
| å¤±æ•—ãƒ†ã‚¹ãƒˆ | {report_data['summary']['failed_tests']} |
| ã‚¹ã‚­ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ | {report_data['summary']['skipped_tests']} |
| ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ | {report_data['summary']['coverage']:.1f}% |
| ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢ | {report_data['summary']['performance_score']:.1f} |
| ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œ | {report_data['summary']['security_issues']} |

## ğŸšª å“è³ªã‚²ãƒ¼ãƒˆçµæœ

**ç·åˆåˆ¤å®š**: {"âœ… PASS" if report_data['quality_gate']['passed'] else "âŒ FAIL"}

"""
        
        # å€‹åˆ¥ãƒ†ã‚¹ãƒˆçµæœ
        md_content += "## ğŸ“‹ å€‹åˆ¥ãƒ†ã‚¹ãƒˆçµæœ\n\n"
        for result in report_data['results']:
            status_emoji = "âœ…" if result['status'] == "PASS" else "âŒ"
            md_content += f"### {status_emoji} {result['name']}\n"
            md_content += f"- **ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: {result['status']}\n"
            md_content += f"- **å®Ÿè¡Œæ™‚é–“**: {result['duration']:.2f}ç§’\n"
            if result.get('coverage'):
                md_content += f"- **ã‚«ãƒãƒ¬ãƒƒã‚¸**: {result['coverage']:.1f}%\n"
            if result.get('error_message'):
                md_content += f"- **ã‚¨ãƒ©ãƒ¼**: {result['error_message']}\n"
            md_content += "\n"
        
        return md_content
    
    async def generate_recommendations(self) -> List[str]:
        """æ”¹å–„ææ¡ˆç”Ÿæˆ"""
        recommendations = []
        
        # ã‚«ãƒãƒ¬ãƒƒã‚¸æ”¹å–„ææ¡ˆ
        if self.stats["coverage"] < self.quality_gate.min_test_coverage:
            recommendations.append(
                f"ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãŒ{self.stats['coverage']:.1f}%ã§ã™ã€‚"
                f"ç›®æ¨™ã®{self.quality_gate.min_test_coverage}%ã«å‘ã‘ã¦ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚"
            )
        
        # å¤±æ•—ãƒ†ã‚¹ãƒˆå¯¾å¿œææ¡ˆ
        if self.stats["failed_tests"] > 0:
            recommendations.append(
                f"{self.stats['failed_tests']}ä»¶ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¦ã„ã¾ã™ã€‚"
                "ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãƒã‚°ä¿®æ­£ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚"
            )
        
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œå¯¾å¿œææ¡ˆ
        if self.stats["security_issues"] > 0:
            recommendations.append(
                f"{self.stats['security_issues']}ä»¶ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚"
                "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒãƒ¼ãƒˆã‚’ç¢ºèªã—ã¦ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚"
            )
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ææ¡ˆ
        if self.stats["performance_score"] < self.quality_gate.min_performance_score:
            recommendations.append(
                f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢ãŒ{self.stats['performance_score']:.1f}ã§ã™ã€‚"
                "ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã¨ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
            )
        
        return recommendations
    
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    async def extract_coverage_info(self) -> float:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸æƒ…å ±æŠ½å‡º"""
        try:
            coverage_file = self.backend_root / "tests" / "reports" / "coverage.json"
            if coverage_file.exists():
                async with aiofiles.open(coverage_file, 'r') as f:
                    data = json.loads(await f.read())
                    return data.get("totals", {}).get("percent_covered", 0.0)
        except:
            pass
        return 0.0
    
    async def count_security_issues(self) -> int:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œæ•°ç®—å‡º"""
        try:
            security_file = self.backend_root / "tests" / "reports" / "security-report.json"
            if security_file.exists():
                async with aiofiles.open(security_file, 'r') as f:
                    data = json.loads(await f.read())
                    return len(data.get("results", []))
        except:
            pass
        return 0
    
    async def calculate_performance_score(self) -> float:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢ç®—å‡º"""
        # ç°¡æ˜“çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢ç®—å‡º
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã€CPUä½¿ç”¨ç‡ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãªã©ã‚’è€ƒæ…®
        return 85.0  # ãƒ‡ãƒ¢å€¤
    
    async def measure_performance_metrics(self) -> Dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™æ¸¬å®š"""
        return {
            "avg_response_time": 0.15,
            "memory_usage_mb": 128.5,
            "cpu_usage_percent": 45.2
        }
    
    async def create_load_test_script(self) -> str:
        """è² è·ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ"""
        script_path = self.backend_root / "tests" / "load_test_script.py"
        
        script_content = '''
from locust import HttpUser, task, between

class ITSMUser(HttpUser):
    wait_time = between(1, 3)
    
    @task(3)
    def test_health(self):
        self.client.get("/health")
    
    @task(2)
    def test_incidents(self):
        self.client.get("/api/v1/incidents/")
    
    @task(1)
    def test_dashboard(self):
        self.client.get("/api/v1/dashboard/stats")
'''
        
        async with aiofiles.open(script_path, 'w') as f:
            await f.write(script_content)
        
        return str(script_path)
    
    def update_stats(self, result: TestResult):
        """çµ±è¨ˆæƒ…å ±æ›´æ–°"""
        self.stats["total_tests"] += 1
        
        if result.status == "PASS":
            self.stats["passed_tests"] += 1
        elif result.status == "FAIL":
            self.stats["failed_tests"] += 1
        else:
            self.stats["skipped_tests"] += 1
        
        self.stats["total_duration"] += result.duration
        
        if result.coverage:
            self.stats["coverage"] = max(self.stats["coverage"], result.coverage)
        
        if result.details and "performance_score" in result.details:
            self.stats["performance_score"] = result.details["performance_score"]
        
        if result.details and "security_issues" in result.details:
            self.stats["security_issues"] += result.details["security_issues"]


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    project_root = Path("/media/kensan/LinuxHDD/ITSM-ITmanagementSystem")
    engine = ComprehensiveTestEngine(project_root)
    
    result = await engine.run_comprehensive_tests()
    
    print("\n" + "="*60)
    print("ğŸ¯ ã€ãƒ•ã‚§ãƒ¼ã‚º2ã€‘åŒ…æ‹¬çš„è‡ªå‹•ãƒ†ã‚¹ãƒˆå®Œäº†")
    print("="*60)
    print(f"âœ… æˆåŠŸ: {result['success']}")
    print(f"ğŸ“Š ç·ãƒ†ã‚¹ãƒˆæ•°: {result['summary']['total_tests']}")
    print(f"âœ… æˆåŠŸãƒ†ã‚¹ãƒˆ: {result['summary']['passed_tests']}")
    print(f"âŒ å¤±æ•—ãƒ†ã‚¹ãƒˆ: {result['summary']['failed_tests']}")
    print(f"ğŸ“ˆ ã‚«ãƒãƒ¬ãƒƒã‚¸: {result['summary']['coverage']:.1f}%")
    print(f"ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: {result['summary']['performance_score']:.1f}")
    print(f"ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œ: {result['summary']['security_issues']}")
    
    if result.get('recommendations'):
        print("\nğŸ“ æ”¹å–„ææ¡ˆ:")
        for i, rec in enumerate(result['recommendations'], 1):
            print(f"  {i}. {rec}")
    
    print(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {result.get('report_path', 'N/A')}")
    print("="*60)
    
    return result


if __name__ == "__main__":
    asyncio.run(main())