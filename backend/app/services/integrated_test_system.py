"""
çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ 
å¼·åŒ–ã•ã‚ŒãŸç„¡é™ãƒ«ãƒ¼ãƒ—è‡ªå‹•ä¿®å¾©ã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆãƒ†ã‚¹ãƒˆãƒ»å‹•ä½œæ¤œè¨¼ãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""

import asyncio
import aiohttp
import aiofiles
import logging
import time
import json
import traceback
import subprocess
import psutil
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
import uuid
import sys
import os

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from enhanced_infinite_loop_monitor import EnhancedInfiniteLoopMonitor, enhanced_monitor
from internal_validation_system import InternalValidationSystem, internal_validator
from enhanced_reporting_system import EnhancedReportingSystem, enhanced_reporter

logger = logging.getLogger(__name__)

class TestPhase(Enum):
    """ãƒ†ã‚¹ãƒˆãƒ•ã‚§ãƒ¼ã‚º"""
    INITIALIZATION = "initialization"
    COMPONENT_TEST = "component_test"
    INTEGRATION_TEST = "integration_test"
    END_TO_END_TEST = "end_to_end_test"
    STRESS_TEST = "stress_test"
    REPORTING = "reporting"
    CLEANUP = "cleanup"

class TestResult(Enum):
    """ãƒ†ã‚¹ãƒˆçµæœ"""
    PASS = "pass"
    FAIL = "fail"
    SKIP = "skip"
    ERROR = "error"

@dataclass
class TestCase:
    """ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å®šç¾©"""
    test_id: str
    name: str
    description: str
    phase: TestPhase
    test_function: str
    timeout: int
    dependencies: List[str]
    critical: bool = False

@dataclass
class TestExecution:
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œçµæœ"""
    test_id: str
    test_name: str
    phase: TestPhase
    start_time: datetime
    end_time: Optional[datetime]
    duration: float
    result: TestResult
    error_message: Optional[str]
    details: Dict[str, Any]
    output_log: List[str]

class IntegratedTestSystem:
    """çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.backend_path = Path("/media/kensan/LinuxHDD/ITSM-ITmanagementSystem/backend")
        self.coordination_path = Path("/media/kensan/LinuxHDD/ITSM-ITmanagementSystem/coordination")
        self.test_results_path = self.coordination_path / "test_results"
        self.test_results_path.mkdir(exist_ok=True)
        
        # ãƒ†ã‚¹ãƒˆçµæœç®¡ç†
        self.test_executions: List[TestExecution] = []
        self.current_session_id = str(uuid.uuid4())
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå‚ç…§
        self.monitor = enhanced_monitor
        self.validator = internal_validator
        self.reporter = enhanced_reporter
        
        # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å®šç¾©
        self.test_cases = self._initialize_test_cases()
    
    def _initialize_test_cases(self) -> List[TestCase]:
        """ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹åˆæœŸåŒ–"""
        test_cases = []
        
        # åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ
        test_cases.extend([
            TestCase(
                test_id="init_001",
                name="ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ",
                description="ã‚·ã‚¹ãƒ†ãƒ ã®åŸºæœ¬åˆæœŸåŒ–ã¨è¨­å®šç¢ºèª",
                phase=TestPhase.INITIALIZATION,
                test_function="_test_system_initialization",
                timeout=30,
                dependencies=[],
                critical=True
            ),
            TestCase(
                test_id="init_002",
                name="ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ",
                description="å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒ­ãƒ¼ãƒ‰ã¨åˆæœŸåŒ–ç¢ºèª",
                phase=TestPhase.INITIALIZATION,
                test_function="_test_component_loading",
                timeout=45,
                dependencies=["init_001"],
                critical=True
            )
        ])
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ†ã‚¹ãƒˆ
        test_cases.extend([
            TestCase(
                test_id="comp_001",
                name="ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ å˜ä½“ãƒ†ã‚¹ãƒˆ",
                description="ç„¡é™ãƒ«ãƒ¼ãƒ—ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®å˜ä½“ãƒ†ã‚¹ãƒˆ",
                phase=TestPhase.COMPONENT_TEST,
                test_function="_test_monitoring_system",
                timeout=120,
                dependencies=["init_002"],
                critical=True
            ),
            TestCase(
                test_id="comp_002",
                name="æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ å˜ä½“ãƒ†ã‚¹ãƒˆ",
                description="å†…éƒ¨æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã®å˜ä½“ãƒ†ã‚¹ãƒˆ",
                phase=TestPhase.COMPONENT_TEST,
                test_function="_test_validation_system",
                timeout=180,
                dependencies=["init_002"],
                critical=True
            ),
            TestCase(
                test_id="comp_003",
                name="ãƒ¬ãƒãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ å˜ä½“ãƒ†ã‚¹ãƒˆ",
                description="å¼·åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®å˜ä½“ãƒ†ã‚¹ãƒˆ",
                phase=TestPhase.COMPONENT_TEST,
                test_function="_test_reporting_system",
                timeout=90,
                dependencies=["init_002"],
                critical=False
            )
        ])
        
        # çµ±åˆãƒ†ã‚¹ãƒˆ
        test_cases.extend([
            TestCase(
                test_id="integ_001",
                name="ç›£è¦–-æ¤œè¨¼é€£æºãƒ†ã‚¹ãƒˆ",
                description="ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã¨æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã®é€£æºãƒ†ã‚¹ãƒˆ",
                phase=TestPhase.INTEGRATION_TEST,
                test_function="_test_monitor_validation_integration",
                timeout=300,
                dependencies=["comp_001", "comp_002"],
                critical=True
            ),
            TestCase(
                test_id="integ_002",
                name="æ¤œè¨¼-ãƒ¬ãƒãƒ¼ãƒˆé€£æºãƒ†ã‚¹ãƒˆ",
                description="æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã¨ãƒ¬ãƒãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®é€£æºãƒ†ã‚¹ãƒˆ",
                phase=TestPhase.INTEGRATION_TEST,
                test_function="_test_validation_reporting_integration",
                timeout=240,
                dependencies=["comp_002", "comp_003"],
                critical=False
            )
        ])
        
        # E2Eãƒ†ã‚¹ãƒˆ
        test_cases.extend([
            TestCase(
                test_id="e2e_001",
                name="ç„¡é™ãƒ«ãƒ¼ãƒ—çµåˆãƒ†ã‚¹ãƒˆ",
                description="å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ±åˆç„¡é™ãƒ«ãƒ¼ãƒ—ãƒ†ã‚¹ãƒˆ",
                phase=TestPhase.END_TO_END_TEST,
                test_function="_test_infinite_loop_e2e",
                timeout=600,
                dependencies=["integ_001", "integ_002"],
                critical=True
            ),
            TestCase(
                test_id="e2e_002",
                name="ã‚¨ãƒ©ãƒ¼ã‚·ãƒŠãƒªã‚ªãƒ†ã‚¹ãƒˆ",
                description="æ„å›³çš„ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã¨è‡ªå‹•ä¿®å¾©ã®ãƒ†ã‚¹ãƒˆ",
                phase=TestPhase.END_TO_END_TEST,
                test_function="_test_error_scenarios",
                timeout=450,
                dependencies=["e2e_001"],
                critical=True
            )
        ])
        
        # ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ
        test_cases.extend([
            TestCase(
                test_id="stress_001",
                name="é«˜è² è·ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ",
                description="é«˜è² è·æ™‚ã®ã‚·ã‚¹ãƒ†ãƒ å®‰å®šæ€§ãƒ†ã‚¹ãƒˆ",
                phase=TestPhase.STRESS_TEST,
                test_function="_test_high_load_stress",
                timeout=900,
                dependencies=["e2e_002"],
                critical=False
            )
        ])
        
        return test_cases
    
    async def run_comprehensive_test_suite(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ"""
        session_start = datetime.now()
        logger.info(f"ğŸš¨ çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆé–‹å§‹ (Session: {self.current_session_id})")
        
        test_session = {
            "session_id": self.current_session_id,
            "start_time": session_start.isoformat(),
            "end_time": None,
            "total_duration": 0.0,
            "phases_executed": [],
            "results": {
                "total_tests": len(self.test_cases),
                "passed": 0,
                "failed": 0,
                "skipped": 0,
                "errors": 0
            },
            "critical_failures": [],
            "phase_results": {},
            "summary": {}
        }
        
        try:
            # ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥å®Ÿè¡Œ
            for phase in TestPhase:
                phase_result = await self._execute_test_phase(phase)
                test_session["phase_results"][phase.value] = phase_result
                test_session["phases_executed"].append(phase.value)
                
                # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
                critical_failures = [
                    exec for exec in phase_result.get("executions", [])
                    if exec.get("critical") and exec.get("result") != TestResult.PASS.value
                ]
                
                if critical_failures:
                    test_session["critical_failures"].extend(critical_failures)
                    logger.error(f"âŒ ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {phase.value}")
                    
                    # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€å¾Œç¶šãƒ•ã‚§ãƒ¼ã‚ºã‚’ã‚¹ã‚­ãƒƒãƒ—
                    remaining_phases = [p for p in TestPhase if p.value not in test_session["phases_executed"]]
                    for skip_phase in remaining_phases:
                        test_session["phase_results"][skip_phase.value] = {
                            "phase": skip_phase.value,
                            "skipped": True,
                            "reason": "Critical failure in previous phase"
                        }
                    break
            
            # çµæœé›†è¨ˆ
            for execution in self.test_executions:
                if execution.result == TestResult.PASS:
                    test_session["results"]["passed"] += 1
                elif execution.result == TestResult.FAIL:
                    test_session["results"]["failed"] += 1
                elif execution.result == TestResult.SKIP:
                    test_session["results"]["skipped"] += 1
                elif execution.result == TestResult.ERROR:
                    test_session["results"]["errors"] += 1
            
            # ã‚µãƒãƒªç”Ÿæˆ
            test_session["summary"] = self._generate_test_summary(test_session)
            
        except Exception as e:
            logger.error(f"âŒ ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            test_session["error"] = str(e)
            test_session["traceback"] = traceback.format_exc()
        
        finally:
            session_end = datetime.now()
            test_session["end_time"] = session_end.isoformat()
            test_session["total_duration"] = (session_end - session_start).total_seconds()
            
            # ãƒ†ã‚¹ãƒˆçµæœä¿å­˜
            await self._save_test_session(test_session)
            
            # çµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            await self._generate_test_report(test_session)
            
            logger.info(f"âœ… çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Œäº† ({test_session['total_duration']:.1f}ç§’)")
        
        return test_session
    
    async def _execute_test_phase(self, phase: TestPhase) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œ"""
        phase_start = datetime.now()
        logger.info(f"ğŸ”„ ãƒ†ã‚¹ãƒˆãƒ•ã‚§ãƒ¼ã‚ºé–‹å§‹: {phase.value}")
        
        phase_tests = [tc for tc in self.test_cases if tc.phase == phase]
        phase_result = {
            "phase": phase.value,
            "start_time": phase_start.isoformat(),
            "end_time": None,
            "duration": 0.0,
            "total_tests": len(phase_tests),
            "passed": 0,
            "failed": 0,
            "skipped": 0,
            "errors": 0,
            "executions": []
        }
        
        try:
            # ä¾å­˜é–¢ä¿‚é †ã§ãƒ†ã‚¹ãƒˆã‚’ã‚½ãƒ¼ãƒˆ
            sorted_tests = self._sort_tests_by_dependencies(phase_tests)
            
            for test_case in sorted_tests:
                execution = await self._execute_single_test(test_case)
                phase_result["executions"].append(asdict(execution))
                
                # çµæœé›†è¨ˆ
                if execution.result == TestResult.PASS:
                    phase_result["passed"] += 1
                elif execution.result == TestResult.FAIL:
                    phase_result["failed"] += 1
                elif execution.result == TestResult.SKIP:
                    phase_result["skipped"] += 1
                elif execution.result == TestResult.ERROR:
                    phase_result["errors"] += 1
                
                # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ†ã‚¹ãƒˆå¤±æ•—æ™‚ã®å‡¦ç†
                if test_case.critical and execution.result not in [TestResult.PASS, TestResult.SKIP]:
                    logger.error(f"âŒ ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ†ã‚¹ãƒˆå¤±æ•—: {test_case.test_id}")
                    break
            
        except Exception as e:
            logger.error(f"ãƒ†ã‚¹ãƒˆãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ ({phase.value}): {e}")
            phase_result["error"] = str(e)
        
        finally:
            phase_end = datetime.now()
            phase_result["end_time"] = phase_end.isoformat()
            phase_result["duration"] = (phase_end - phase_start).total_seconds()
            
            logger.info(f"âœ… ãƒ†ã‚¹ãƒˆãƒ•ã‚§ãƒ¼ã‚ºå®Œäº†: {phase.value} ({phase_result['duration']:.1f}ç§’)")
        
        return phase_result
    
    def _sort_tests_by_dependencies(self, tests: List[TestCase]) -> List[TestCase]:
        """ä¾å­˜é–¢ä¿‚é †ã§ãƒ†ã‚¹ãƒˆã‚’ã‚½ãƒ¼ãƒˆ"""
        sorted_tests = []
        remaining_tests = tests.copy()
        executed_test_ids = set()
        
        while remaining_tests:
            progress_made = False
            
            for test in remaining_tests[:]:
                if all(dep in executed_test_ids for dep in test.dependencies):
                    sorted_tests.append(test)
                    executed_test_ids.add(test.test_id)
                    remaining_tests.remove(test)
                    progress_made = True
            
            if not progress_made:
                # å¾ªç’°ä¾å­˜é–¢ä¿‚ã®å¯èƒ½æ€§
                logger.warning("å¾ªç’°ä¾å­˜é–¢ä¿‚ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚æ®‹ã‚Šã®ãƒ†ã‚¹ãƒˆã‚’ä¾å­˜é–¢ä¿‚ãªã—ã§å®Ÿè¡Œã—ã¾ã™ã€‚")
                sorted_tests.extend(remaining_tests)
                break
        
        return sorted_tests
    
    async def _execute_single_test(self, test_case: TestCase) -> TestExecution:
        """å˜ä¸€ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        start_time = datetime.now()
        output_log = []
        
        execution = TestExecution(
            test_id=test_case.test_id,
            test_name=test_case.name,
            phase=test_case.phase,
            start_time=start_time,
            end_time=None,
            duration=0.0,
            result=TestResult.ERROR,
            error_message=None,
            details={},
            output_log=output_log
        )
        
        try:
            logger.info(f"â–¶ï¸ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ: {test_case.test_id} - {test_case.name}")
            output_log.append(f"Test started: {test_case.test_id}")
            
            # ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯
            if not self._check_dependencies(test_case.dependencies):
                execution.result = TestResult.SKIP
                execution.error_message = "Dependencies not satisfied"
                output_log.append("Test skipped: Dependencies not satisfied")
                return execution
            
            # ãƒ†ã‚¹ãƒˆé–¢æ•°å–å¾—ã¨å®Ÿè¡Œ
            test_function = getattr(self, test_case.test_function, None)
            if test_function is None:
                execution.result = TestResult.ERROR
                execution.error_message = f"Test function {test_case.test_function} not found"
                output_log.append(f"Error: Test function not found - {test_case.test_function}")
                return execution
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            test_result = await asyncio.wait_for(
                test_function(test_case, output_log),
                timeout=test_case.timeout
            )
            
            if isinstance(test_result, dict):
                execution.result = TestResult(test_result.get("result", "error"))
                execution.details = test_result.get("details", {})
                if "error" in test_result:
                    execution.error_message = test_result["error"]
            else:
                execution.result = TestResult.PASS if test_result else TestResult.FAIL
            
            output_log.append(f"Test completed: {execution.result.value}")
            
        except asyncio.TimeoutError:
            execution.result = TestResult.ERROR
            execution.error_message = f"Test timeout ({test_case.timeout}s)"
            output_log.append(f"Error: Test timeout after {test_case.timeout} seconds")
        except Exception as e:
            execution.result = TestResult.ERROR
            execution.error_message = str(e)
            output_log.append(f"Error: {str(e)}")
            logger.error(f"Test execution error ({test_case.test_id}): {e}")
        
        finally:
            end_time = datetime.now()
            execution.end_time = end_time
            execution.duration = (end_time - start_time).total_seconds()
            
            # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œçµæœã‚’å±¥æ­´ã«è¿½åŠ 
            self.test_executions.append(execution)
            
            result_symbol = {
                TestResult.PASS: "âœ…",
                TestResult.FAIL: "âŒ",
                TestResult.SKIP: "â­ï¸",
                TestResult.ERROR: "ğŸš¨"
            }.get(execution.result, "â“")
            
            logger.info(f"{result_symbol} ãƒ†ã‚¹ãƒˆå®Œäº†: {test_case.test_id} ({execution.duration:.2f}s) - {execution.result.value}")
        
        return execution
    
    def _check_dependencies(self, dependencies: List[str]) -> bool:
        """ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯"""
        if not dependencies:
            return True
        
        passed_test_ids = {
            exec.test_id for exec in self.test_executions 
            if exec.result == TestResult.PASS
        }
        
        return all(dep in passed_test_ids for dep in dependencies)
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè£…é–¢æ•°ç¾¤
    async def _test_system_initialization(self, test_case: TestCase, output_log: List[str]) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
        try:
            output_log.append("Initializing system components...")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ç¢ºèª
            required_paths = [
                self.backend_path,
                self.coordination_path,
                self.test_results_path
            ]
            
            for path in required_paths:
                if not path.exists():
                    return {
                        "result": "fail",
                        "error": f"Required path does not exist: {path}"
                    }
                output_log.append(f"Path exists: {path}")
            
            # åŸºæœ¬è¨­å®šç¢ºèª
            output_log.append("System initialization successful")
            return {
                "result": "pass",
                "details": {
                    "backend_path": str(self.backend_path),
                    "coordination_path": str(self.coordination_path),
                    "test_results_path": str(self.test_results_path)
                }
            }
            
        except Exception as e:
            return {
                "result": "error",
                "error": str(e)
            }
    
    async def _test_component_loading(self, test_case: TestCase, output_log: List[str]) -> Dict[str, Any]:
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ"""
        try:
            output_log.append("Loading system components...")
            
            # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å­˜åœ¨ç¢ºèª
            components = {
                "monitor": self.monitor,
                "validator": self.validator,
                "reporter": self.reporter
            }
            
            for name, component in components.items():
                if component is None:
                    return {
                        "result": "fail",
                        "error": f"Component {name} is not loaded"
                    }
                output_log.append(f"Component loaded: {name}")
            
            # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åŸºæœ¬ãƒ¡ã‚½ãƒƒãƒ‰ç¢ºèª
            if not hasattr(self.monitor, 'get_status'):
                return {
                    "result": "fail",
                    "error": "Monitor component missing get_status method"
                }
            
            if not hasattr(self.validator, 'get_validation_status'):
                return {
                    "result": "fail",
                    "error": "Validator component missing get_validation_status method"
                }
            
            output_log.append("All components loaded successfully")
            return {
                "result": "pass",
                "details": {
                    "loaded_components": list(components.keys())
                }
            }
            
        except Exception as e:
            return {
                "result": "error",
                "error": str(e)
            }
    
    async def _test_monitoring_system(self, test_case: TestCase, output_log: List[str]) -> Dict[str, Any]:
        """ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ å˜ä½“ãƒ†ã‚¹ãƒˆ"""
        try:
            output_log.append("Testing monitoring system...")
            
            # ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®çŠ¶æ…‹å–å¾—
            status = self.monitor.get_status()
            if not isinstance(status, dict):
                return {
                    "result": "fail",
                    "error": "Monitor status is not a dictionary"
                }
            
            output_log.append(f"Monitor status retrieved: {len(status)} fields")
            
            # åŸºæœ¬ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
            required_fields = ["monitoring", "loop_count", "total_errors_fixed"]
            for field in required_fields:
                if field not in status:
                    return {
                        "result": "fail",
                        "error": f"Missing required field in status: {field}"
                    }
                output_log.append(f"Required field present: {field}")
            
            # ã‚¨ãƒ©ãƒ¼æ¤œçŸ¥æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆï¼ˆç°¡å˜ãªæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆï¼‰
            if hasattr(self.monitor, '_detect_api_errors'):
                output_log.append("Error detection method available")
            
            output_log.append("Monitoring system test passed")
            return {
                "result": "pass",
                "details": {
                    "status_fields": list(status.keys()),
                    "loop_count": status.get("loop_count", 0),
                    "errors_fixed": status.get("total_errors_fixed", 0)
                }
            }
            
        except Exception as e:
            return {
                "result": "error",
                "error": str(e)
            }
    
    async def _test_validation_system(self, test_case: TestCase, output_log: List[str]) -> Dict[str, Any]:
        """æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ å˜ä½“ãƒ†ã‚¹ãƒˆ"""
        try:
            output_log.append("Testing validation system...")
            
            # æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã®çŠ¶æ…‹å–å¾—
            status = self.validator.get_validation_status()
            if not isinstance(status, dict):
                return {
                    "result": "fail",
                    "error": "Validator status is not a dictionary"
                }
            
            output_log.append(f"Validator status retrieved: {len(status)} fields")
            
            # åŸºæœ¬ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
            required_fields = ["available_suites"]
            for field in required_fields:
                if field not in status:
                    return {
                        "result": "fail",
                        "error": f"Missing required field in status: {field}"
                    }
                output_log.append(f"Required field present: {field}")
            
            # æ¤œè¨¼ã‚¹ã‚¤ãƒ¼ãƒˆã®å­˜åœ¨ç¢ºèª
            available_suites = status.get("available_suites", [])
            if not available_suites:
                return {
                    "result": "fail",
                    "error": "No validation suites available"
                }
            
            output_log.append(f"Available validation suites: {len(available_suites)}")
            
            # ç°¡å˜ãªæ¤œè¨¼ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            try:
                test_result = await self.validator.run_comprehensive_validation(["api_functionality"])
                if "overall_score" in test_result:
                    output_log.append(f"Validation test completed: score {test_result['overall_score']}")
                else:
                    output_log.append("Validation test completed without score")
            except Exception as e:
                output_log.append(f"Validation test failed: {str(e)}")
                # æ¤œè¨¼ãƒ†ã‚¹ãƒˆã®å¤±æ•—ã¯çµæœã«å½±éŸ¿ã—ãªã„ï¼ˆã‚·ã‚¹ãƒ†ãƒ ã¯å‹•ä½œã—ã¦ã„ã‚‹ï¼‰
            
            output_log.append("Validation system test passed")
            return {
                "result": "pass",
                "details": {
                    "available_suites": available_suites,
                    "total_sessions": status.get("total_sessions", 0),
                    "total_validations": status.get("total_validations", 0)
                }
            }
            
        except Exception as e:
            return {
                "result": "error",
                "error": str(e)
            }
    
    async def _test_reporting_system(self, test_case: TestCase, output_log: List[str]) -> Dict[str, Any]:
        """ãƒ¬ãƒãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ å˜ä½“ãƒ†ã‚¹ãƒˆ"""
        try:
            output_log.append("Testing reporting system...")
            
            # ãƒ¬ãƒãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®çŠ¶æ…‹å–å¾—
            status = self.reporter.get_report_status()
            if not isinstance(status, dict):
                return {
                    "result": "fail",
                    "error": "Reporter status is not a dictionary"
                }
            
            output_log.append(f"Reporter status retrieved: {len(status)} fields")
            
            # åŸºæœ¬ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
            required_fields = ["available_formats", "reports_directory"]
            for field in required_fields:
                if field not in status:
                    return {
                        "result": "fail",
                        "error": f"Missing required field in status: {field}"
                    }
                output_log.append(f"Required field present: {field}")
            
            # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
            reports_dir = Path(status.get("reports_directory", ""))
            if not reports_dir.exists():
                return {
                    "result": "fail",
                    "error": f"Reports directory does not exist: {reports_dir}"
                }
            
            output_log.append(f"Reports directory exists: {reports_dir}")
            
            # åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆç¢ºèª
            available_formats = status.get("available_formats", [])
            if not available_formats:
                return {
                    "result": "fail",
                    "error": "No report formats available"
                }
            
            output_log.append(f"Available report formats: {available_formats}")
            
            output_log.append("Reporting system test passed")
            return {
                "result": "pass",
                "details": {
                    "available_formats": available_formats,
                    "reports_directory": str(reports_dir),
                    "total_reports": status.get("total_reports", 0)
                }
            }
            
        except Exception as e:
            return {
                "result": "error",
                "error": str(e)
            }
    
    async def _test_monitor_validation_integration(self, test_case: TestCase, output_log: List[str]) -> Dict[str, Any]:
        """ç›£è¦–-æ¤œè¨¼é€£æºãƒ†ã‚¹ãƒˆ"""
        try:
            output_log.append("Testing monitor-validation integration...")
            
            # ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
            monitor_status = self.monitor.get_status()
            output_log.append("Monitor status retrieved")
            
            # æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã§ç°¡å˜ãªæ¤œè¨¼å®Ÿè¡Œ
            validation_result = await self.validator.run_comprehensive_validation(["api_functionality"])
            output_log.append("Validation executed")
            
            # çµæœã®æ•´åˆæ€§ç¢ºèª
            if not isinstance(validation_result, dict):
                return {
                    "result": "fail",
                    "error": "Validation result is not a dictionary"
                }
            
            # åŸºæœ¬çš„ãªé€£æºç¢ºèª
            integration_score = 100.0
            
            if "overall_score" not in validation_result:
                integration_score -= 30
                output_log.append("Warning: Missing overall_score in validation result")
            
            if "success_rate" not in validation_result:
                integration_score -= 20
                output_log.append("Warning: Missing success_rate in validation result")
            
            success = integration_score >= 70.0
            result_type = "pass" if success else "fail"
            
            output_log.append(f"Monitor-validation integration test: {result_type} (score: {integration_score})")
            return {
                "result": result_type,
                "details": {
                    "integration_score": integration_score,
                    "monitor_status_fields": len(monitor_status),
                    "validation_result_fields": len(validation_result)
                }
            }
            
        except Exception as e:
            return {
                "result": "error",
                "error": str(e)
            }
    
    async def _test_validation_reporting_integration(self, test_case: TestCase, output_log: List[str]) -> Dict[str, Any]:
        """æ¤œè¨¼-ãƒ¬ãƒãƒ¼ãƒˆé€£æºãƒ†ã‚¹ãƒˆ"""
        try:
            output_log.append("Testing validation-reporting integration...")
            
            # æ¤œè¨¼å®Ÿè¡Œ
            validation_result = await self.validator.run_comprehensive_validation(["api_functionality"])
            output_log.append("Validation executed for reporting integration test")
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆï¼ˆJSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰
            from enhanced_reporting_system import ReportFormat
            report_result = await self.reporter.generate_comprehensive_report(
                report_format=ReportFormat.JSON,
                include_charts=False
            )
            output_log.append("Report generation executed")
            
            # çµæœç¢ºèª
            if not isinstance(report_result, dict):
                return {
                    "result": "fail",
                    "error": "Report result is not a dictionary"
                }
            
            if "report_id" not in report_result:
                return {
                    "result": "fail",
                    "error": "Missing report_id in report result"
                }
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆç¢ºèª
            if "file_path" in report_result:
                report_file = Path(report_result["file_path"])
                if report_file.exists():
                    output_log.append(f"Report file generated: {report_file}")
                else:
                    output_log.append("Warning: Report file not found")
            
            output_log.append("Validation-reporting integration test passed")
            return {
                "result": "pass",
                "details": {
                    "validation_score": validation_result.get("overall_score", 0),
                    "report_id": report_result.get("report_id"),
                    "report_generated": "file_path" in report_result
                }
            }
            
        except Exception as e:
            return {
                "result": "error",
                "error": str(e)
            }
    
    async def _test_infinite_loop_e2e(self, test_case: TestCase, output_log: List[str]) -> Dict[str, Any]:
        """ç„¡é™ãƒ«ãƒ¼ãƒ—E2Eãƒ†ã‚¹ãƒˆ"""
        try:
            output_log.append("Testing infinite loop end-to-end functionality...")
            
            # ç°¡å˜ãªç„¡é™ãƒ«ãƒ¼ãƒ—ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ5ã‚µã‚¤ã‚¯ãƒ«ï¼‰
            simulation_cycles = 5
            successful_cycles = 0
            
            for cycle in range(simulation_cycles):
                output_log.append(f"Simulating cycle {cycle + 1}/{simulation_cycles}")
                
                try:
                    # ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹å–å¾—
                    monitor_status = self.monitor.get_status()
                    
                    # æ¤œè¨¼å®Ÿè¡Œ
                    validation_result = await self.validator.run_comprehensive_validation(["api_functionality"])
                    
                    # ã‚µã‚¤ã‚¯ãƒ«æˆåŠŸåˆ¤å®š
                    if isinstance(monitor_status, dict) and isinstance(validation_result, dict):
                        successful_cycles += 1
                        output_log.append(f"Cycle {cycle + 1} completed successfully")
                    else:
                        output_log.append(f"Cycle {cycle + 1} failed")
                    
                    # ã‚µã‚¤ã‚¯ãƒ«é–“ã®å°ä¼‘æ­¢
                    await asyncio.sleep(2)
                    
                except Exception as e:
                    output_log.append(f"Cycle {cycle + 1} error: {str(e)}")
            
            # çµæœè©•ä¾¡
            success_rate = (successful_cycles / simulation_cycles) * 100
            success = success_rate >= 80.0  # 80%ä»¥ä¸Šã®æˆåŠŸç‡ã§ãƒ‘ã‚¹
            
            output_log.append(f"E2E test completed: {successful_cycles}/{simulation_cycles} cycles successful ({success_rate}%)")
            
            return {
                "result": "pass" if success else "fail",
                "details": {
                    "total_cycles": simulation_cycles,
                    "successful_cycles": successful_cycles,
                    "success_rate": success_rate
                }
            }
            
        except Exception as e:
            return {
                "result": "error",
                "error": str(e)
            }
    
    async def _test_error_scenarios(self, test_case: TestCase, output_log: List[str]) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ã‚·ãƒŠãƒªã‚ªãƒ†ã‚¹ãƒˆ"""
        try:
            output_log.append("Testing error scenarios and recovery...")
            
            # ã‚¨ãƒ©ãƒ¼ã‚·ãƒŠãƒªã‚ªã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            scenarios_tested = 0
            scenarios_handled = 0
            
            # ã‚·ãƒŠãƒªã‚ª1: ç„¡åŠ¹ãªURLã§ã®æ¤œè¨¼
            try:
                output_log.append("Scenario 1: Testing with invalid URL")
                
                # ã‚ªãƒªã‚¸ãƒŠãƒ«URLã‚’ä¿å­˜
                original_url = self.validator.base_url
                
                # ç„¡åŠ¹ãªURLã«å¤‰æ›´
                self.validator.base_url = "http://invalid-url-for-testing:9999"
                
                # æ¤œè¨¼å®Ÿè¡Œï¼ˆã‚¨ãƒ©ãƒ¼ãŒæœŸå¾…ã•ã‚Œã‚‹ï¼‰
                validation_result = await self.validator.run_comprehensive_validation(["api_functionality"])
                
                # URLã‚’å¾©å…ƒ
                self.validator.base_url = original_url
                
                scenarios_tested += 1
                if isinstance(validation_result, dict):
                    scenarios_handled += 1  # ã‚¨ãƒ©ãƒ¼ãŒé©åˆ‡ã«å‡¦ç†ã•ã‚ŒãŸ
                    output_log.append("Scenario 1: Error handled gracefully")
                
            except Exception as e:
                output_log.append(f"Scenario 1: Unexpected error - {str(e)}")
            
            # ã‚·ãƒŠãƒªã‚ª2: å­˜åœ¨ã—ãªã„æ¤œè¨¼ã‚¹ã‚¤ãƒ¼ãƒˆ
            try:
                output_log.append("Scenario 2: Testing with non-existent validation suite")
                
                validation_result = await self.validator.run_comprehensive_validation(["non_existent_suite"])
                
                scenarios_tested += 1
                if isinstance(validation_result, dict):
                    scenarios_handled += 1
                    output_log.append("Scenario 2: Non-existent suite handled gracefully")
                
            except Exception as e:
                output_log.append(f"Scenario 2: Unexpected error - {str(e)}")
            
            # ã‚·ãƒŠãƒªã‚ª3: ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆè»½å¾®ï¼‰
            try:
                output_log.append("Scenario 3: Testing memory usage monitoring")
                
                # ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
                memory_percent = psutil.virtual_memory().percent
                output_log.append(f"Current memory usage: {memory_percent:.1f}%")
                
                scenarios_tested += 1
                scenarios_handled += 1  # ãƒ¡ãƒ¢ãƒªç›£è¦–ãŒæ©Ÿèƒ½ã—ã¦ã„ã‚‹
                
            except Exception as e:
                output_log.append(f"Scenario 3: Error - {str(e)}")
            
            # çµæœè©•ä¾¡
            if scenarios_tested == 0:
                return {
                    "result": "fail",
                    "error": "No scenarios were tested"
                }
            
            handling_rate = (scenarios_handled / scenarios_tested) * 100
            success = handling_rate >= 70.0  # 70%ä»¥ä¸Šã®ã‚·ãƒŠãƒªã‚ªã§ã‚¨ãƒ©ãƒ¼ãŒé©åˆ‡ã«å‡¦ç†ã•ã‚Œã‚‹
            
            output_log.append(f"Error scenarios test: {scenarios_handled}/{scenarios_tested} scenarios handled ({handling_rate}%)")
            
            return {
                "result": "pass" if success else "fail",
                "details": {
                    "scenarios_tested": scenarios_tested,
                    "scenarios_handled": scenarios_handled,
                    "handling_rate": handling_rate
                }
            }
            
        except Exception as e:
            return {
                "result": "error",
                "error": str(e)
            }
    
    async def _test_high_load_stress(self, test_case: TestCase, output_log: List[str]) -> Dict[str, Any]:
        """é«˜è² è·ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ"""
        try:
            output_log.append("Testing high load stress scenarios...")
            
            # è»½å¾®ãªã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆï¼ˆåŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™ï¼‰
            concurrent_tests = 3
            test_duration = 30  # ç§’
            
            # åŒæ™‚å®Ÿè¡Œã‚¿ã‚¹ã‚¯ä½œæˆ
            async def stress_task(task_id: int):
                start_time = time.time()
                successful_operations = 0
                
                while time.time() - start_time < test_duration:
                    try:
                        # ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹å–å¾—
                        status = self.monitor.get_status()
                        if isinstance(status, dict):
                            successful_operations += 1
                        
                        await asyncio.sleep(1)  # 1ç§’é–“éš”
                        
                    except Exception:
                        pass  # ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼ˆã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆã®ãŸã‚ï¼‰
                
                return successful_operations
            
            # åŒæ™‚ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
            tasks = [stress_task(i) for i in range(concurrent_tests)]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # çµæœé›†è¨ˆ
            total_operations = 0
            successful_tasks = 0
            
            for i, result in enumerate(results):
                if isinstance(result, int):
                    total_operations += result
                    successful_tasks += 1
                    output_log.append(f"Stress task {i+1}: {result} successful operations")
                else:
                    output_log.append(f"Stress task {i+1}: failed with {type(result).__name__}")
            
            # ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆè©•ä¾¡
            task_success_rate = (successful_tasks / concurrent_tests) * 100
            avg_operations = total_operations / successful_tasks if successful_tasks > 0 else 0
            
            # æˆåŠŸæ¡ä»¶: 70%ä»¥ä¸Šã®ã‚¿ã‚¹ã‚¯ãŒæˆåŠŸã—ã€å¹³å‡æ“ä½œæ•°ãŒ10ä»¥ä¸Š
            success = task_success_rate >= 70.0 and avg_operations >= 10
            
            output_log.append(f"Stress test completed: {task_success_rate}% task success, {avg_operations:.1f} avg operations")
            
            return {
                "result": "pass" if success else "fail",
                "details": {
                    "concurrent_tests": concurrent_tests,
                    "test_duration": test_duration,
                    "successful_tasks": successful_tasks,
                    "total_operations": total_operations,
                    "task_success_rate": task_success_rate,
                    "avg_operations_per_task": avg_operations
                }
            }
            
        except Exception as e:
            return {
                "result": "error",
                "error": str(e)
            }
    
    def _generate_test_summary(self, test_session: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆã‚µãƒãƒªç”Ÿæˆ"""
        try:
            results = test_session.get("results", {})
            total_tests = results.get("total_tests", 0)
            passed = results.get("passed", 0)
            
            if total_tests == 0:
                success_rate = 0.0
                overall_status = "no_tests"
            else:
                success_rate = (passed / total_tests) * 100
                
                if success_rate >= 90:
                    overall_status = "excellent"
                elif success_rate >= 80:
                    overall_status = "good"
                elif success_rate >= 70:
                    overall_status = "acceptable"
                elif success_rate >= 50:
                    overall_status = "poor"
                else:
                    overall_status = "critical"
            
            critical_failures = test_session.get("critical_failures", [])
            has_critical_failures = len(critical_failures) > 0
            
            return {
                "overall_status": overall_status,
                "success_rate": success_rate,
                "has_critical_failures": has_critical_failures,
                "critical_failure_count": len(critical_failures),
                "phases_completed": len(test_session.get("phases_executed", [])),
                "total_phases": len(TestPhase),
                "execution_time_minutes": test_session.get("total_duration", 0) / 60,
                "recommended_actions": self._generate_recommendations(test_session)
            }
            
        except Exception as e:
            logger.error(f"ãƒ†ã‚¹ãƒˆã‚µãƒãƒªç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _generate_recommendations(self, test_session: Dict[str, Any]) -> List[str]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        try:
            results = test_session.get("results", {})
            critical_failures = test_session.get("critical_failures", [])
            
            if critical_failures:
                recommendations.append("ğŸš¨ ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚å„ªå…ˆçš„ã«ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚")
            
            failed_tests = results.get("failed", 0)
            if failed_tests > 0:
                recommendations.append(f"âŒ {failed_tests}ä»¶ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¦ã„ã¾ã™ã€‚ãƒ†ã‚¹ãƒˆãƒ­ã‚°ã‚’ç¢ºèªã—ã¦åŸå› ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚")
            
            error_tests = results.get("errors", 0)
            if error_tests > 0:
                recommendations.append(f"ğŸš¨ {error_tests}ä»¶ã®ãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚ã‚·ã‚¹ãƒ†ãƒ è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            
            total_duration = test_session.get("total_duration", 0)
            if total_duration > 3600:  # 1æ™‚é–“ä»¥ä¸Š
                recommendations.append("â±ï¸ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“ãŒé•·ã„ã§ã™ã€‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
            
            success_rate = (results.get("passed", 0) / results.get("total_tests", 1)) * 100
            if success_rate >= 95:
                recommendations.append("ğŸ‰ ãƒ†ã‚¹ãƒˆçµæœãŒå„ªç§€ã§ã™ã€‚ã‚·ã‚¹ãƒ†ãƒ ã¯å®‰å®šã—ã¦å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")
            elif success_rate >= 80:
                recommendations.append("âœ… ãƒ†ã‚¹ãƒˆçµæœã¯è‰¯å¥½ã§ã™ã€‚å°ã•ãªæ”¹å–„ã§ã•ã‚‰ã«å‘ä¸Šã§ãã¾ã™ã€‚")
            
            if not recommendations:
                recommendations.append("ğŸ“Š ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚è©³ç´°ãªçµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            
        except Exception as e:
            recommendations.append(f"â“ æ¨å¥¨äº‹é …ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return recommendations
    
    async def _save_test_session(self, test_session: Dict[str, Any]):
        """ãƒ†ã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            session_file = self.test_results_path / f"test_session_{timestamp}.json"
            
            async with aiofiles.open(session_file, 'w') as f:
                await f.write(json.dumps(test_session, indent=2, ensure_ascii=False, default=str))
            
            # æœ€æ–°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯æ›´æ–°
            latest_file = self.test_results_path / "latest_test_session.json"
            if latest_file.exists():
                latest_file.unlink()
            
            try:
                latest_file.symlink_to(session_file.name)
            except:
                # ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ãŒä½œæˆã§ããªã„å ´åˆã¯ã‚³ãƒ”ãƒ¼
                async with aiofiles.open(latest_file, 'w') as f:
                    await f.write(json.dumps(test_session, indent=2, ensure_ascii=False, default=str))
            
            logger.info(f"ğŸ’¾ ãƒ†ã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜å®Œäº†: {session_file}")
            
        except Exception as e:
            logger.error(f"ãƒ†ã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def _generate_test_report(self, test_session: Dict[str, Any]):
        """ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        try:
            # ç°¡å˜ãªãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            from enhanced_reporting_system import ReportFormat
            
            # ãƒ†ã‚¹ãƒˆçµæœã‚’ãƒ¬ãƒãƒ¼ã‚¿ãƒ¼ã«é€£æº
            test_report = await self.reporter.generate_comprehensive_report(
                report_format=ReportFormat.HTML,
                include_charts=True
            )
            
            logger.info(f"ğŸ“‹ ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {test_report.get('file_path', 'ã‚¨ãƒ©ãƒ¼')}")
            
        except Exception as e:
            logger.error(f"ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_test_status(self) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆçŠ¶æ³å–å¾—"""
        try:
            return {
                "current_session_id": self.current_session_id,
                "total_test_cases": len(self.test_cases),
                "total_executions": len(self.test_executions),
                "test_results_directory": str(self.test_results_path),
                "available_phases": [phase.value for phase in TestPhase],
                "execution_summary": {
                    "passed": len([e for e in self.test_executions if e.result == TestResult.PASS]),
                    "failed": len([e for e in self.test_executions if e.result == TestResult.FAIL]),
                    "skipped": len([e for e in self.test_executions if e.result == TestResult.SKIP]),
                    "errors": len([e for e in self.test_executions if e.result == TestResult.ERROR])
                }
            }
        except Exception as e:
            logger.error(f"ãƒ†ã‚¹ãƒˆçŠ¶æ³å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
integrated_tester = IntegratedTestSystem()

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        logger.info("ğŸš¨ çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œé–‹å§‹")
        
        # åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ
        test_result = await integrated_tester.run_comprehensive_test_suite()
        
        print("\n" + "="*60)
        print("ğŸ“‹ çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œçµæœ")
        print("="*60)
        
        # åŸºæœ¬çµæœ
        results = test_result.get("results", {})
        print(f"ğŸ“Š ç·ãƒ†ã‚¹ãƒˆæ•°: {results.get('total_tests', 0)}")
        print(f"âœ… æˆåŠŸ: {results.get('passed', 0)}")
        print(f"âŒ å¤±æ•—: {results.get('failed', 0)}")
        print(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {results.get('skipped', 0)}")
        print(f"ğŸš¨ ã‚¨ãƒ©ãƒ¼: {results.get('errors', 0)}")
        
        # ã‚µãƒãƒª
        summary = test_result.get("summary", {})
        print(f"\nğŸ¯ å…¨ä½“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {summary.get('overall_status', 'unknown')}")
        print(f"ğŸ“Š æˆåŠŸç‡: {summary.get('success_rate', 0):.1f}%")
        print(f"â±ï¸ å®Ÿè¡Œæ™‚é–“: {summary.get('execution_time_minutes', 0):.1f}åˆ†")
        
        # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¨ãƒ©ãƒ¼
        critical_failures = test_result.get("critical_failures", [])
        if critical_failures:
            print(f"\nğŸš¨ ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¨ãƒ©ãƒ¼: {len(critical_failures)}ä»¶")
        
        # æ¨å¥¨äº‹é …
        recommendations = summary.get("recommended_actions", [])
        if recommendations:
            print("\nğŸ’¡ æ¨å¥¨äº‹é …:")
            for rec in recommendations:
                print(f"  - {rec}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
        print(f"\nğŸ’¾ ãƒ†ã‚¹ãƒˆçµæœãƒ•ã‚¡ã‚¤ãƒ«: {integrated_tester.test_results_path / 'latest_test_session.json'}")
        print(f"ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {integrated_tester.reporter.reports_path}")
        
        print("\n" + "="*60)
        print("âœ… çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œå®Œäº†")
        print("="*60)
        
    except KeyboardInterrupt:
        logger.info("âŒ¨ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler("/media/kensan/LinuxHDD/ITSM-ITmanagementSystem/coordination/integrated_test.log"),
            logging.StreamHandler()
        ]
    )
    
    asyncio.run(main())