name: Test Suite - Comprehensive ITSM Testing

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run comprehensive test suite daily at 3 AM JST
    - cron: '0 18 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: false
        default: 'all'
        type: choice
        options:
        - all
        - unit
        - api
        - e2e
        - load
        - security
      skip_heavy_tests:
        description: 'Skip heavy tests (load, performance)'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  PLAYWRIGHT_BROWSERS_PATH: /ms-playwright
  ITSM_TEST_MODE: 'true'
  HEADLESS: 'true'
  RECORD_VIDEO: 'true'

jobs:
  pre-flight-check:
    runs-on: ubuntu-latest
    outputs:
      should-run-tests: ${{ steps.decision.outputs.run_tests }}
      test-strategy: ${{ steps.decision.outputs.strategy }}
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 2

    - name: Test strategy decision
      id: decision
      run: |
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          echo "run_tests=true" >> $GITHUB_OUTPUT
          echo "strategy=${{ github.event.inputs.test_type }}" >> $GITHUB_OUTPUT
        elif [ "${{ github.event_name }}" = "schedule" ]; then
          echo "run_tests=true" >> $GITHUB_OUTPUT
          echo "strategy=all" >> $GITHUB_OUTPUT
        else
          # Check for code changes that require testing
          changed_files=$(git diff --name-only HEAD^ HEAD)
          if echo "$changed_files" | grep -E '\.(py|js|ts|html|css|yml|yaml)$'; then
            echo "run_tests=true" >> $GITHUB_OUTPUT
            echo "strategy=auto" >> $GITHUB_OUTPUT
          else
            echo "run_tests=false" >> $GITHUB_OUTPUT
            echo "strategy=skip" >> $GITHUB_OUTPUT
          fi
        fi

  setup-test-environment:
    runs-on: ubuntu-latest
    needs: pre-flight-check
    if: needs.pre-flight-check.outputs.should-run-tests == 'true'
    outputs:
      test-db-ready: ${{ steps.db-setup.outputs.ready }}
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-test.txt
        pip install -r backend/requirements.txt

    - name: Setup test database
      id: db-setup
      env:
        DATABASE_URL: sqlite:///test.db
        ASYNC_DATABASE_URL: sqlite+aiosqlite:///test.db
        SECRET_KEY: ${{ secrets.SECRET_KEY || 'test-secret-key-for-github-actions-testing' }}
        ENCRYPTION_KEY: ${{ secrets.ENCRYPTION_KEY || 'test-encryption-key-32-chars-12345' }}
        ITSM_TEST_MODE: true
      run: |
        cd backend
        python init_sqlite_db.py
        echo "ready=true" >> $GITHUB_OUTPUT

    - name: Cache test environment
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/pip
          backend/test_async.db
        key: test-env-${{ runner.os }}-${{ hashFiles('**/requirements*.txt') }}

  unit-tests:
    runs-on: ubuntu-latest
    needs: [pre-flight-check, setup-test-environment]
    if: |
      needs.pre-flight-check.outputs.should-run-tests == 'true' &&
      (needs.pre-flight-check.outputs.test-strategy == 'all' || 
       needs.pre-flight-check.outputs.test-strategy == 'unit' || 
       needs.pre-flight-check.outputs.test-strategy == 'auto')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-test.txt
        pip install -r backend/requirements.txt

    - name: Create test environment file
      run: |
        cp .env.test .env
        cp .env.test backend/.env.test
        echo "ITSM_TEST_MODE=true" >> .env
        echo "ITSM_TEST_MODE=true" >> backend/.env.test

    - name: Run unit tests
      env:
        DATABASE_URL: sqlite:///test.db
        ASYNC_DATABASE_URL: sqlite+aiosqlite:///test.db
        SECRET_KEY: ${{ secrets.SECRET_KEY || 'test-secret-key-for-github-actions-testing' }}
        ENCRYPTION_KEY: ${{ secrets.ENCRYPTION_KEY || 'test-encryption-key-32-chars-12345' }}
        ITSM_TEST_MODE: true
      run: |
        mkdir -p tests/reports
        cd backend
        python init_sqlite_db.py
        cd ..
        pytest backend/tests/unit/ -v --tb=short \
          --html=tests/reports/unit-report.html \
          --json-report --json-report-file=tests/reports/unit-report.json \
          --cov=backend/app --cov-report=json:tests/reports/unit-coverage.json \
          --cov-report=html:tests/reports/coverage_html \
          -m "unit" || true

    - name: Upload unit test reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: unit-test-reports
        path: |
          tests/reports/unit-*.html
          tests/reports/unit-*.json
          tests/reports/coverage_html/
        retention-days: 30

  api-tests:
    runs-on: ubuntu-latest
    needs: [pre-flight-check, setup-test-environment]
    if: |
      needs.pre-flight-check.outputs.should-run-tests == 'true' &&
      (needs.pre-flight-check.outputs.test-strategy == 'all' || 
       needs.pre-flight-check.outputs.test-strategy == 'api' || 
       needs.pre-flight-check.outputs.test-strategy == 'auto')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-test.txt
        pip install -r backend/requirements.txt

    - name: Create test environment
      run: |
        cp .env.test .env
        cp .env.test backend/.env.test
        echo "ITSM_TEST_MODE=true" >> .env
        echo "ITSM_TEST_MODE=true" >> backend/.env.test

    - name: Initialize test database
      env:
        DATABASE_URL: sqlite:///test.db
        ASYNC_DATABASE_URL: sqlite+aiosqlite:///test.db
        SECRET_KEY: ${{ secrets.SECRET_KEY || 'test-secret-key-for-github-actions-testing' }}
        ENCRYPTION_KEY: ${{ secrets.ENCRYPTION_KEY || 'test-encryption-key-32-chars-12345' }}
        ITSM_TEST_MODE: true
      run: |
        cd backend
        python init_sqlite_db.py

    - name: Start backend server for API tests
      env:
        DATABASE_URL: sqlite:///test.db
        ASYNC_DATABASE_URL: sqlite+aiosqlite:///test.db
        SECRET_KEY: ${{ secrets.SECRET_KEY || 'test-secret-key-for-github-actions-testing' }}
        ENCRYPTION_KEY: ${{ secrets.ENCRYPTION_KEY || 'test-encryption-key-32-chars-12345' }}
        ITSM_TEST_MODE: true
      run: |
        cd backend
        python start_server.py &
        sleep 15
        echo "BACKEND_PID=$!" >> $GITHUB_ENV

    - name: Wait for server readiness
      run: |
        timeout 60 bash -c 'until curl -s http://localhost:8000/health; do sleep 2; done'

    - name: Run API tests
      run: |
        mkdir -p tests/reports
        pytest tests/api/ -v --tb=short \
          --html=tests/reports/api-report.html \
          --json-report --json-report-file=tests/reports/api-report.json \
          --cov=backend/app --cov-report=json:tests/reports/api-coverage.json \
          --benchmark-json=tests/reports/api-benchmark.json \
          -m "api and not slow" || true

    - name: Stop backend server
      if: always()
      run: |
        if [ -n "$BACKEND_PID" ]; then 
          kill $BACKEND_PID || true
          sleep 5
        fi
        pkill -f "python start_server.py" || true

    - name: Upload API test reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: api-test-reports
        path: |
          tests/reports/api-*.html
          tests/reports/api-*.json
        retention-days: 30

  e2e-tests:
    runs-on: ubuntu-latest
    needs: [pre-flight-check, setup-test-environment, api-tests]
    if: |
      needs.pre-flight-check.outputs.should-run-tests == 'true' &&
      (needs.pre-flight-check.outputs.test-strategy == 'all' || 
       needs.pre-flight-check.outputs.test-strategy == 'e2e' || 
       needs.pre-flight-check.outputs.test-strategy == 'auto')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-test.txt
        pip install -r backend/requirements.txt

    - name: Install frontend dependencies
      run: |
        cd frontend
        npm ci

    - name: Install Playwright browsers
      run: |
        python -m playwright install chromium
        sudo apt-get update
        sudo apt-get install -y libasound2t64 libatk-bridge2.0-0 libdrm2 libxkbcommon0 libxcomposite1 libxdamage1 libxrandr2 libgbm1 libxss1
        python -m playwright install-deps chromium

    - name: Create test environment
      run: |
        cp .env.test .env
        cp .env.test backend/.env.test
        echo "HEADLESS=true" >> .env
        echo "RECORD_VIDEO=true" >> .env
        echo "ITSM_TEST_MODE=true" >> .env
        echo "ITSM_TEST_MODE=true" >> backend/.env.test

    - name: Initialize test database
      env:
        DATABASE_URL: sqlite:///test.db
        ASYNC_DATABASE_URL: sqlite+aiosqlite:///test.db
        SECRET_KEY: ${{ secrets.SECRET_KEY || 'test-secret-key-for-github-actions-testing' }}
        ENCRYPTION_KEY: ${{ secrets.ENCRYPTION_KEY || 'test-encryption-key-32-chars-12345' }}
        ITSM_TEST_MODE: true
      run: |
        cd backend
        python init_sqlite_db.py

    - name: Start backend server
      env:
        DATABASE_URL: sqlite:///test.db
        ASYNC_DATABASE_URL: sqlite+aiosqlite:///test.db
        SECRET_KEY: ${{ secrets.SECRET_KEY || 'test-secret-key-for-github-actions-testing' }}
        ENCRYPTION_KEY: ${{ secrets.ENCRYPTION_KEY || 'test-encryption-key-32-chars-12345' }}
        ITSM_TEST_MODE: true
      run: |
        cd backend
        python start_server.py &
        sleep 15
        echo "BACKEND_PID=$!" >> $GITHUB_ENV

    - name: Build and start frontend server
      run: |
        cd frontend
        npm run build
        npm run preview &
        sleep 15
        echo "FRONTEND_PID=$!" >> $GITHUB_ENV

    - name: Wait for servers to be ready
      run: |
        timeout 120 bash -c 'until curl -s http://localhost:8000/health; do sleep 3; done'
        timeout 120 bash -c 'until curl -s http://localhost:3000; do sleep 3; done'

    - name: Run E2E tests with Playwright
      run: |
        mkdir -p tests/reports/videos tests/reports/screenshots
        pytest tests/e2e/ -v --tb=short \
          --html=tests/reports/e2e-report.html \
          --json-report --json-report-file=tests/reports/e2e-report.json \
          -m "e2e and not slow" \
          --maxfail=5 || true

    - name: Run Playwright E2E tests
      run: |
        cd frontend
        npx playwright test --config=../playwright.config.ts \
          --reporter=html:../tests/reports/playwright-report.html \
          --reporter=json:../tests/reports/playwright-report.json || true

    - name: Stop servers
      if: always()
      run: |
        if [ -n "$BACKEND_PID" ]; then kill $BACKEND_PID || true; fi
        if [ -n "$FRONTEND_PID" ]; then kill $FRONTEND_PID || true; fi
        pkill -f "python start_server.py" || true
        pkill -f "npm run preview" || true

    - name: Upload E2E test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: e2e-test-reports
        path: |
          tests/reports/e2e-*.html
          tests/reports/e2e-*.json
          tests/reports/playwright-*.html
          tests/reports/playwright-*.json
          tests/reports/videos/
          tests/reports/screenshots/
          test-results/
        retention-days: 30

  load-tests:
    runs-on: ubuntu-latest
    needs: [pre-flight-check, setup-test-environment, api-tests]
    if: |
      needs.pre-flight-check.outputs.should-run-tests == 'true' &&
      (needs.pre-flight-check.outputs.test-strategy == 'all' || 
       needs.pre-flight-check.outputs.test-strategy == 'load') &&
      github.event.inputs.skip_heavy_tests != 'true'
    timeout-minutes: 45
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-test.txt
        pip install -r backend/requirements.txt

    - name: Create test environment
      run: |
        cp .env.test .env
        echo "BENCHMARK_ROUNDS=3" >> .env
        echo "BENCHMARK_TIMEOUT=600" >> .env
        echo "ITSM_TEST_MODE=true" >> .env

    - name: Initialize test database
      env:
        DATABASE_URL: sqlite:///test.db
        ASYNC_DATABASE_URL: sqlite+aiosqlite:///test.db
        SECRET_KEY: ${{ secrets.SECRET_KEY || 'test-secret-key-for-github-actions-testing' }}
        ENCRYPTION_KEY: ${{ secrets.ENCRYPTION_KEY || 'test-encryption-key-32-chars-12345' }}
        ITSM_TEST_MODE: true
      run: |
        cd backend
        python init_sqlite_db.py

    - name: Start backend server for load tests
      env:
        DATABASE_URL: sqlite:///test.db
        ASYNC_DATABASE_URL: sqlite+aiosqlite:///test.db
        SECRET_KEY: ${{ secrets.SECRET_KEY || 'test-secret-key-for-github-actions-testing' }}
        ENCRYPTION_KEY: ${{ secrets.ENCRYPTION_KEY || 'test-encryption-key-32-chars-12345' }}
        ITSM_TEST_MODE: true
      run: |
        cd backend
        python start_server.py &
        sleep 15
        echo "BACKEND_PID=$!" >> $GITHUB_ENV

    - name: Wait for server readiness
      run: |
        timeout 60 bash -c 'until curl -s http://localhost:8000/health; do sleep 2; done'

    - name: Run load tests
      run: |
        mkdir -p tests/reports
        pytest tests/load/ -v --tb=short \
          --html=tests/reports/load-report.html \
          --json-report --json-report-file=tests/reports/load-report.json \
          --benchmark-json=tests/reports/load-benchmark.json \
          -m "load" \
          --benchmark-sort=mean || true

    - name: Stop backend server
      if: always()
      run: |
        if [ -n "$BACKEND_PID" ]; then kill $BACKEND_PID || true; fi
        pkill -f "python start_server.py" || true

    - name: Upload load test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: load-test-reports
        path: |
          tests/reports/load-*.html
          tests/reports/load-*.json
        retention-days: 30

  security-tests:
    runs-on: ubuntu-latest
    needs: [pre-flight-check, setup-test-environment]
    if: |
      needs.pre-flight-check.outputs.should-run-tests == 'true' &&
      (needs.pre-flight-check.outputs.test-strategy == 'all' || 
       needs.pre-flight-check.outputs.test-strategy == 'security' || 
       needs.pre-flight-check.outputs.test-strategy == 'auto')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety semgrep

    - name: Run Bandit security scan
      continue-on-error: true
      run: |
        mkdir -p tests/reports
        bandit -r backend/app/ -f json -o tests/reports/bandit-report.json -ll || true
        bandit -r backend/app/ -f txt -ll || true

    - name: Run dependency vulnerability check
      continue-on-error: true
      run: |
        safety check --json > tests/reports/safety-report.json || true
        safety check || true

    - name: Run Semgrep security scan
      continue-on-error: true
      run: |
        semgrep --config=auto backend/ --json > tests/reports/semgrep-report.json || true

    - name: Upload security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports
        path: tests/reports/*-report.json
        retention-days: 30

  test-consolidation:
    runs-on: ubuntu-latest
    needs: [unit-tests, api-tests, e2e-tests, load-tests, security-tests]
    if: always() && needs.pre-flight-check.outputs.should-run-tests == 'true'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install report dependencies
      run: |
        python -m pip install --upgrade pip
        pip install jinja2 matplotlib pandas plotly

    - name: Download all test reports
      uses: actions/download-artifact@v4
      with:
        path: artifacts/

    - name: Generate consolidated test report
      run: |
        mkdir -p tests/reports/consolidated
        python tests/utils/generate_consolidated_report.py \
          --input-dir artifacts/ \
          --output-dir tests/reports/consolidated/ || true

    - name: Upload consolidated report
      uses: actions/upload-artifact@v4
      with:
        name: consolidated-test-suite-report
        path: tests/reports/consolidated/
        retention-days: 60

  quality-gate-evaluation:
    runs-on: ubuntu-latest
    needs: [test-consolidation]
    if: always() && needs.pre-flight-check.outputs.should-run-tests == 'true'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download consolidated reports
      uses: actions/download-artifact@v4
      with:
        path: artifacts/

    - name: Quality Gate Assessment
      id: quality-gate
      run: |
        # Define quality gates
        MIN_COVERAGE=75
        MAX_FAILED_TESTS=2
        MAX_SECURITY_HIGH_ISSUES=0
        MAX_SECURITY_MEDIUM_ISSUES=5
        
        # Initialize counters
        TOTAL_FAILED=0
        TOTAL_COVERAGE=0
        HIGH_SECURITY_ISSUES=0
        MEDIUM_SECURITY_ISSUES=0
        QUALITY_GATE_PASSED=true
        
        # Check unit test results
        if [ -f "artifacts/unit-test-reports/unit-report.json" ]; then
          UNIT_FAILED=$(jq -r '.summary.failed // 0' artifacts/unit-test-reports/unit-report.json)
          TOTAL_FAILED=$((TOTAL_FAILED + UNIT_FAILED))
        fi
        
        # Check API test results
        if [ -f "artifacts/api-test-reports/api-report.json" ]; then
          API_FAILED=$(jq -r '.summary.failed // 0' artifacts/api-test-reports/api-report.json)
          TOTAL_FAILED=$((TOTAL_FAILED + API_FAILED))
        fi
        
        # Check E2E test results
        if [ -f "artifacts/e2e-test-reports/e2e-report.json" ]; then
          E2E_FAILED=$(jq -r '.summary.failed // 0' artifacts/e2e-test-reports/e2e-report.json)
          TOTAL_FAILED=$((TOTAL_FAILED + E2E_FAILED))
        fi
        
        # Check coverage
        if [ -f "artifacts/api-test-reports/api-coverage.json" ]; then
          COVERAGE=$(jq -r '.totals.percent_covered // 0' artifacts/api-test-reports/api-coverage.json)
          TOTAL_COVERAGE=$(echo "$COVERAGE" | cut -d. -f1)
        fi
        
        # Check security issues
        if [ -f "artifacts/security-reports/bandit-report.json" ]; then
          HIGH_SECURITY_ISSUES=$(jq -r '[.results[] | select(.issue_severity == "HIGH")] | length' artifacts/security-reports/bandit-report.json 2>/dev/null || echo 0)
          MEDIUM_SECURITY_ISSUES=$(jq -r '[.results[] | select(.issue_severity == "MEDIUM")] | length' artifacts/security-reports/bandit-report.json 2>/dev/null || echo 0)
        fi
        
        echo "🔍 Quality Gate Evaluation:"
        echo "- Failed Tests: $TOTAL_FAILED (max: $MAX_FAILED_TESTS)"
        echo "- Coverage: $TOTAL_COVERAGE% (min: $MIN_COVERAGE%)"
        echo "- High Security Issues: $HIGH_SECURITY_ISSUES (max: $MAX_SECURITY_HIGH_ISSUES)"
        echo "- Medium Security Issues: $MEDIUM_SECURITY_ISSUES (max: $MAX_SECURITY_MEDIUM_ISSUES)"
        
        # Evaluate quality gates
        if [ "$TOTAL_FAILED" -gt "$MAX_FAILED_TESTS" ]; then
          echo "❌ Quality Gate FAILED: Too many test failures ($TOTAL_FAILED > $MAX_FAILED_TESTS)"
          QUALITY_GATE_PASSED=false
        fi
        
        if [ "$TOTAL_COVERAGE" -lt "$MIN_COVERAGE" ]; then
          echo "❌ Quality Gate FAILED: Insufficient test coverage ($TOTAL_COVERAGE% < $MIN_COVERAGE%)"
          QUALITY_GATE_PASSED=false
        fi
        
        if [ "$HIGH_SECURITY_ISSUES" -gt "$MAX_SECURITY_HIGH_ISSUES" ]; then
          echo "❌ Quality Gate FAILED: Too many high security issues ($HIGH_SECURITY_ISSUES > $MAX_SECURITY_HIGH_ISSUES)"
          QUALITY_GATE_PASSED=false
        fi
        
        if [ "$MEDIUM_SECURITY_ISSUES" -gt "$MAX_SECURITY_MEDIUM_ISSUES" ]; then
          echo "❌ Quality Gate FAILED: Too many medium security issues ($MEDIUM_SECURITY_ISSUES > $MAX_SECURITY_MEDIUM_ISSUES)"
          QUALITY_GATE_PASSED=false
        fi
        
        if [ "$QUALITY_GATE_PASSED" = "true" ]; then
          echo "✅ Quality Gate PASSED: All criteria met"
          echo "quality_gate_result=passed" >> $GITHUB_OUTPUT
        else
          echo "❌ Quality Gate FAILED: Some criteria not met"
          echo "quality_gate_result=failed" >> $GITHUB_OUTPUT
        fi
        
        # Export metrics for reporting
        echo "total_failed=$TOTAL_FAILED" >> $GITHUB_OUTPUT
        echo "total_coverage=$TOTAL_COVERAGE" >> $GITHUB_OUTPUT
        echo "high_security_issues=$HIGH_SECURITY_ISSUES" >> $GITHUB_OUTPUT
        echo "medium_security_issues=$MEDIUM_SECURITY_ISSUES" >> $GITHUB_OUTPUT

    - name: Comment quality gate results on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const qualityGateResult = '${{ steps.quality-gate.outputs.quality_gate_result }}';
          const totalFailed = '${{ steps.quality-gate.outputs.total_failed }}';
          const totalCoverage = '${{ steps.quality-gate.outputs.total_coverage }}';
          const highSecurityIssues = '${{ steps.quality-gate.outputs.high_security_issues }}';
          const mediumSecurityIssues = '${{ steps.quality-gate.outputs.medium_security_issues }}';
          
          const icon = qualityGateResult === 'passed' ? '✅' : '❌';
          const status = qualityGateResult === 'passed' ? 'PASSED' : 'FAILED';
          
          const body = `## 🧪 Test Suite Quality Gate ${icon}
          
          **Quality Gate Status**: ${status}
          
          ### 📊 Test Results Summary
          | Metric | Value | Status |
          |--------|-------|--------|
          | Failed Tests | ${totalFailed} | ${totalFailed <= 2 ? '✅' : '❌'} |
          | Test Coverage | ${totalCoverage}% | ${totalCoverage >= 75 ? '✅' : '❌'} |
          | High Security Issues | ${highSecurityIssues} | ${highSecurityIssues == 0 ? '✅' : '❌'} |
          | Medium Security Issues | ${mediumSecurityIssues} | ${mediumSecurityIssues <= 5 ? '✅' : '❌'} |
          
          ### 📋 Quality Criteria
          - ✅ Max Failed Tests: 2
          - ✅ Min Test Coverage: 75%
          - ✅ Max High Security Issues: 0
          - ✅ Max Medium Security Issues: 5
          
          [📊 View Detailed Test Reports](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
          `;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: body
          });

  test-suite-notification:
    runs-on: ubuntu-latest
    needs: [quality-gate-evaluation]
    if: always() && (github.event_name == 'schedule' || failure())
    
    steps:
    - name: Notify on test suite failure
      if: failure() || needs.quality-gate-evaluation.outputs.quality_gate_result == 'failed'
      uses: actions/github-script@v6
      with:
        script: |
          const title = '🚨 ITSM Test Suite Failed';
          const body = `
          ## Test Suite Failure Alert
          
          **Repository**: ${{ github.repository }}
          **Branch**: ${{ github.ref }}
          **Commit**: ${{ github.sha }}
          **Workflow Run**: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
          **Trigger**: ${{ github.event_name }}
          
          ### Required Actions
          1. Review failed test reports
          2. Fix identified issues
          3. Re-run test suite
          4. Ensure quality gates pass
          
          **Priority**: High - Immediate investigation required
          `;
          
          // Create an issue for failed scheduled runs
          if ('${{ github.event_name }}' === 'schedule') {
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: title,
              body: body,
              labels: ['bug', 'test-failure', 'automated', 'high-priority']
            });
          }

    - name: Notify on successful test suite
      if: success() && needs.quality-gate-evaluation.outputs.quality_gate_result == 'passed'
      run: |
        echo "✅ Test Suite completed successfully - All quality gates passed"
        echo "📊 Test coverage, security, and functionality standards met"