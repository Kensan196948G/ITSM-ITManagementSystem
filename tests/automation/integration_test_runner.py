#!/usr/bin/env python3
"""
çµ±åˆãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼
- GitHub Actionsè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- Pytestã€Playwrightã€API ãƒ†ã‚¹ãƒˆã®çµ±åˆ
- CI/CDå“è³ªã‚²ãƒ¼ãƒˆæ¤œè¨¼
- æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""

import asyncio
import json
import subprocess
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any
import sys
import os
import tempfile
import shutil

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent.parent))

# ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from tests.automation.github_actions_test_suite import GitHubActionsTestSuite
from tests.automation.pytest_integration_suite import TestGitHubActionsAutomation
from tests.automation.playwright_e2e_suite import PlaywrightE2ETestSuite


class IntegrationTestRunner:
    """çµ±åˆãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼"""

    def __init__(self):
        self.base_path = Path(__file__).parent
        self.project_root = self.base_path.parent.parent
        self.start_time = datetime.now()

        # ãƒ†ã‚¹ãƒˆçµæœ
        self.test_suites = {}
        self.overall_results = {
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 0,
            "error_tests": 0,
            "warning_tests": 0,
            "skipped_tests": 0,
        }

        # å“è³ªã‚²ãƒ¼ãƒˆåŸºæº–
        self.quality_gates = {
            "min_success_rate": 0.80,  # 80%ä»¥ä¸Šã®æˆåŠŸç‡
            "max_duration_minutes": 15,  # 15åˆ†ä»¥å†…ã§ã®å®Œäº†
            "required_test_categories": [
                "github_api",
                "error_detection",
                "auto_repair",
                "integration",
            ],
            "max_critical_failures": 0,  # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«å¤±æ•—ã¯0ä»¶
            "min_coverage_percentage": 70,  # 70%ä»¥ä¸Šã®ã‚«ãƒãƒ¬ãƒƒã‚¸
        }

    async def setup_test_environment(self):
        """ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        print("ğŸ› ï¸ Setting up integration test environment...")

        # ãƒ†ã‚¹ãƒˆçµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        test_dirs = ["reports", "logs", "screenshots", "artifacts"]

        for dir_name in test_dirs:
            (self.base_path / dir_name).mkdir(exist_ok=True)

        # ãƒ†ã‚¹ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
        test_config = {
            "environment": "test",
            "github_repo": "Kensan196948G/ITSM-ITManagementSystem",
            "test_timeout": 30,
            "max_retries": 2,
            "enable_screenshots": True,
            "enable_videos": False,
            "parallel_execution": False,
        }

        config_file = self.base_path / "test_config.json"
        with open(config_file, "w") as f:
            json.dump(test_config, f, indent=2)

        print("âœ… Test environment setup completed")

    async def run_github_actions_test_suite(self) -> Dict[str, Any]:
        """GitHub Actions ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ"""
        print("\n" + "=" * 60)
        print("ğŸ” Running GitHub Actions Test Suite")
        print("=" * 60)

        try:
            suite = GitHubActionsTestSuite()
            results = await suite.run_all_tests()

            self.test_suites["github_actions"] = results
            return results

        except Exception as e:
            error_result = {
                "test_suite": "GitHub Actions Test Suite",
                "status": "ERROR",
                "message": str(e),
                "timestamp": datetime.now().isoformat(),
            }
            self.test_suites["github_actions"] = error_result
            print(f"ğŸ’¥ GitHub Actions test suite failed: {e}")
            return error_result

    async def run_pytest_integration_suite(self) -> Dict[str, Any]:
        """Pytestçµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ"""
        print("\n" + "=" * 60)
        print("ğŸ§ª Running Pytest Integration Suite")
        print("=" * 60)

        try:
            # Pytestã‚’åˆ¥ãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œ
            pytest_command = [
                "python",
                "-m",
                "pytest",
                str(self.base_path / "pytest_integration_suite.py"),
                "-v",
                "--tb=short",
                "--maxfail=10",
                "--durations=10",
                f"--junitxml={self.base_path}/reports/pytest_results.xml",
                f"--json-report",
                f"--json-report-file={self.base_path}/reports/pytest_results.json",
            ]

            result = subprocess.run(
                pytest_command,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=600,  # 10åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            )

            # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è©³ç´°ã‚’èª­ã¿å–ã‚Š
            pytest_results_file = self.base_path / "reports" / "pytest_results.json"
            if pytest_results_file.exists():
                with open(pytest_results_file, "r") as f:
                    pytest_data = json.load(f)

                suite_results = {
                    "test_suite": "Pytest Integration Suite",
                    "status": "PASS" if result.returncode == 0 else "FAIL",
                    "total_tests": pytest_data.get("summary", {}).get("total", 0),
                    "passed": pytest_data.get("summary", {}).get("passed", 0),
                    "failed": pytest_data.get("summary", {}).get("failed", 0),
                    "errors": pytest_data.get("summary", {}).get("error", 0),
                    "skipped": pytest_data.get("summary", {}).get("skipped", 0),
                    "duration": pytest_data.get("duration", 0),
                    "success_rate": pytest_data.get("summary", {}).get("passed", 0)
                    / max(pytest_data.get("summary", {}).get("total", 1), 1),
                    "output": result.stdout,
                    "timestamp": datetime.now().isoformat(),
                }
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ¨™æº–å‡ºåŠ›ã‹ã‚‰çµæœã‚’è§£æ
                suite_results = {
                    "test_suite": "Pytest Integration Suite",
                    "status": "PASS" if result.returncode == 0 else "FAIL",
                    "return_code": result.returncode,
                    "output": result.stdout,
                    "error": result.stderr,
                    "timestamp": datetime.now().isoformat(),
                }

            self.test_suites["pytest_integration"] = suite_results
            return suite_results

        except subprocess.TimeoutExpired:
            timeout_result = {
                "test_suite": "Pytest Integration Suite",
                "status": "TIMEOUT",
                "message": "Test execution timed out after 10 minutes",
                "timestamp": datetime.now().isoformat(),
            }
            self.test_suites["pytest_integration"] = timeout_result
            return timeout_result

        except Exception as e:
            error_result = {
                "test_suite": "Pytest Integration Suite",
                "status": "ERROR",
                "message": str(e),
                "timestamp": datetime.now().isoformat(),
            }
            self.test_suites["pytest_integration"] = error_result
            print(f"ğŸ’¥ Pytest integration suite failed: {e}")
            return error_result

    async def run_playwright_e2e_suite(self) -> Dict[str, Any]:
        """Playwright E2Eãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ"""
        print("\n" + "=" * 60)
        print("ğŸ­ Running Playwright E2E Suite")
        print("=" * 60)

        try:
            suite = PlaywrightE2ETestSuite()
            results = await suite.run_all_e2e_tests()

            self.test_suites["playwright_e2e"] = results
            return results

        except Exception as e:
            error_result = {
                "test_suite": "Playwright E2E Suite",
                "status": "ERROR",
                "message": str(e),
                "timestamp": datetime.now().isoformat(),
            }
            self.test_suites["playwright_e2e"] = error_result
            print(f"ğŸ’¥ Playwright E2E suite failed: {e}")
            return error_result

    async def run_api_health_checks(self) -> Dict[str, Any]:
        """APIå¥å…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        print("\n" + "=" * 60)
        print("ğŸ¥ Running API Health Checks")
        print("=" * 60)

        health_checks = []

        # GitHub APIæ¥ç¶šãƒ†ã‚¹ãƒˆ
        try:
            github_result = subprocess.run(
                ["gh", "api", "user"], capture_output=True, text=True, timeout=30
            )

            health_checks.append(
                {
                    "name": "GitHub API Connection",
                    "status": "PASS" if github_result.returncode == 0 else "FAIL",
                    "details": (
                        github_result.stdout
                        if github_result.returncode == 0
                        else github_result.stderr
                    ),
                }
            )
        except Exception as e:
            health_checks.append(
                {"name": "GitHub API Connection", "status": "ERROR", "details": str(e)}
            )

        # ãƒ­ãƒ¼ã‚«ãƒ«APIå¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
        api_ports = [8000, 5000, 3000]
        for port in api_ports:
            try:
                import requests

                response = requests.get(f"http://localhost:{port}/health", timeout=5)
                health_checks.append(
                    {
                        "name": f"Local API Port {port}",
                        "status": "PASS" if response.status_code == 200 else "FAIL",
                        "details": f"Status: {response.status_code}",
                    }
                )
                break  # æœ€åˆã«æˆåŠŸã—ãŸãƒãƒ¼ãƒˆã§åœæ­¢
            except Exception:
                health_checks.append(
                    {
                        "name": f"Local API Port {port}",
                        "status": "SKIP",
                        "details": "Not running or not accessible",
                    }
                )

        # çµæœã‚µãƒãƒªãƒ¼
        passed_checks = len([c for c in health_checks if c["status"] == "PASS"])
        total_checks = len(health_checks)

        api_results = {
            "test_suite": "API Health Checks",
            "status": "PASS" if passed_checks > 0 else "FAIL",
            "total_checks": total_checks,
            "passed_checks": passed_checks,
            "health_checks": health_checks,
            "timestamp": datetime.now().isoformat(),
        }

        self.test_suites["api_health"] = api_results
        return api_results

    async def run_load_performance_tests(self) -> Dict[str, Any]:
        """è² è·ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        print("\n" + "=" * 60)
        print("âš¡ Running Load & Performance Tests")
        print("=" * 60)

        try:
            # ç°¡å˜ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
            from coordination.error_pattern_analyzer import ErrorPatternAnalyzer
            from coordination.auto_repair_engine import AutoRepairEngine

            performance_results = []

            # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
            analyzer = ErrorPatternAnalyzer()
            large_log = "\n".join(
                [
                    f"Error {i}: ModuleNotFoundError: No module named 'module{i}'"
                    for i in range(1000)
                ]
            )

            start_time = time.time()
            matches = analyzer.analyze_log_content(large_log)
            analysis_duration = time.time() - start_time

            performance_results.append(
                {
                    "test": "Error Pattern Analysis",
                    "input_size": "1000 lines",
                    "duration": analysis_duration,
                    "throughput": 1000 / analysis_duration,
                    "status": "PASS" if analysis_duration < 5.0 else "WARN",
                }
            )

            # ä¿®å¾©ã‚¨ãƒ³ã‚¸ãƒ³ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
            repair_engine = AutoRepairEngine()
            start_time = time.time()

            # è»½é‡ãªãƒ†ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³
            test_patterns = ["config validation", "lint check"]
            repair_results = await repair_engine.smart_repair(test_patterns)

            repair_duration = time.time() - start_time

            performance_results.append(
                {
                    "test": "Smart Repair Engine",
                    "input_size": f"{len(test_patterns)} patterns",
                    "duration": repair_duration,
                    "status": "PASS" if repair_duration < 30.0 else "WARN",
                }
            )

            # å…¨ä½“çš„ãªåˆ¤å®š
            passed_tests = len(
                [r for r in performance_results if r["status"] == "PASS"]
            )
            total_tests = len(performance_results)

            load_results = {
                "test_suite": "Load & Performance Tests",
                "status": "PASS" if passed_tests >= total_tests * 0.8 else "WARN",
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "performance_results": performance_results,
                "timestamp": datetime.now().isoformat(),
            }

            self.test_suites["load_performance"] = load_results
            return load_results

        except Exception as e:
            error_result = {
                "test_suite": "Load & Performance Tests",
                "status": "ERROR",
                "message": str(e),
                "timestamp": datetime.now().isoformat(),
            }
            self.test_suites["load_performance"] = error_result
            return error_result

    def calculate_overall_metrics(self):
        """å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        for suite_name, suite_results in self.test_suites.items():
            if isinstance(suite_results, dict):
                # å„ã‚¹ã‚¤ãƒ¼ãƒˆã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
                if "total_tests" in suite_results:
                    self.overall_results["total_tests"] += suite_results["total_tests"]
                    self.overall_results["passed_tests"] += suite_results.get(
                        "passed", 0
                    )
                    self.overall_results["failed_tests"] += suite_results.get(
                        "failed", 0
                    )
                    self.overall_results["error_tests"] += suite_results.get(
                        "errors", 0
                    )
                    self.overall_results["skipped_tests"] += suite_results.get(
                        "skipped", 0
                    )
                elif "total_checks" in suite_results:
                    self.overall_results["total_tests"] += suite_results["total_checks"]
                    self.overall_results["passed_tests"] += suite_results.get(
                        "passed_checks", 0
                    )
                elif "status" in suite_results:
                    # å˜ä¸€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®å ´åˆ
                    self.overall_results["total_tests"] += 1
                    if suite_results["status"] == "PASS":
                        self.overall_results["passed_tests"] += 1
                    elif suite_results["status"] == "FAIL":
                        self.overall_results["failed_tests"] += 1
                    elif suite_results["status"] == "ERROR":
                        self.overall_results["error_tests"] += 1

    def check_quality_gates(self) -> Dict[str, Any]:
        """å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯"""
        print("\n" + "=" * 60)
        print("ğŸšª Checking Quality Gates")
        print("=" * 60)

        quality_checks = []
        overall_status = "PASS"

        # 1. æˆåŠŸç‡ãƒã‚§ãƒƒã‚¯
        total_tests = self.overall_results["total_tests"]
        passed_tests = self.overall_results["passed_tests"]
        success_rate = passed_tests / total_tests if total_tests > 0 else 0

        success_rate_check = {
            "gate": "Minimum Success Rate",
            "threshold": f">= {self.quality_gates['min_success_rate']:.1%}",
            "actual": f"{success_rate:.1%}",
            "status": (
                "PASS"
                if success_rate >= self.quality_gates["min_success_rate"]
                else "FAIL"
            ),
        }
        quality_checks.append(success_rate_check)

        if success_rate_check["status"] == "FAIL":
            overall_status = "FAIL"

        # 2. å®Ÿè¡Œæ™‚é–“ãƒã‚§ãƒƒã‚¯
        duration = (datetime.now() - self.start_time).total_seconds() / 60
        duration_check = {
            "gate": "Maximum Duration",
            "threshold": f"<= {self.quality_gates['max_duration_minutes']} minutes",
            "actual": f"{duration:.1f} minutes",
            "status": (
                "PASS"
                if duration <= self.quality_gates["max_duration_minutes"]
                else "FAIL"
            ),
        }
        quality_checks.append(duration_check)

        if duration_check["status"] == "FAIL":
            overall_status = "FAIL"

        # 3. å¿…è¦ãƒ†ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒªãƒã‚§ãƒƒã‚¯
        available_categories = list(self.test_suites.keys())
        required_categories = self.quality_gates["required_test_categories"]
        missing_categories = [
            cat for cat in required_categories if cat not in available_categories
        ]

        categories_check = {
            "gate": "Required Test Categories",
            "threshold": f"All of: {required_categories}",
            "actual": f"Available: {available_categories}",
            "missing": missing_categories,
            "status": "PASS" if len(missing_categories) == 0 else "WARN",
        }
        quality_checks.append(categories_check)

        # 4. ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«å¤±æ•—ãƒã‚§ãƒƒã‚¯
        critical_failures = self.overall_results["error_tests"]
        critical_check = {
            "gate": "Maximum Critical Failures",
            "threshold": f"<= {self.quality_gates['max_critical_failures']}",
            "actual": str(critical_failures),
            "status": (
                "PASS"
                if critical_failures <= self.quality_gates["max_critical_failures"]
                else "FAIL"
            ),
        }
        quality_checks.append(critical_check)

        if critical_check["status"] == "FAIL":
            overall_status = "FAIL"

        quality_gate_results = {
            "overall_status": overall_status,
            "checks": quality_checks,
            "summary": {
                "total_tests": total_tests,
                "success_rate": success_rate,
                "duration_minutes": duration,
                "critical_failures": critical_failures,
            },
            "timestamp": datetime.now().isoformat(),
        }

        # çµæœå‡ºåŠ›
        for check in quality_checks:
            status_emoji = (
                "âœ…"
                if check["status"] == "PASS"
                else "âš ï¸" if check["status"] == "WARN" else "âŒ"
            )
            print(
                f"{status_emoji} {check['gate']}: {check['actual']} (Required: {check['threshold']})"
            )

        print(f"\nğŸ† Overall Quality Gate Status: {overall_status}")

        return quality_gate_results

    async def generate_comprehensive_report(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("\n" + "=" * 60)
        print("ğŸ“Š Generating Comprehensive Report")
        print("=" * 60)

        # å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        self.calculate_overall_metrics()

        # å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
        quality_gate_results = self.check_quality_gates()

        duration = (datetime.now() - self.start_time).total_seconds()

        comprehensive_report = {
            "report_metadata": {
                "generated_at": datetime.now().isoformat(),
                "duration_seconds": duration,
                "test_environment": "integration",
                "report_version": "1.0",
            },
            "executive_summary": {
                "overall_status": quality_gate_results["overall_status"],
                "total_test_suites": len(self.test_suites),
                "total_tests": self.overall_results["total_tests"],
                "passed_tests": self.overall_results["passed_tests"],
                "failed_tests": self.overall_results["failed_tests"],
                "error_tests": self.overall_results["error_tests"],
                "success_rate": self.overall_results["passed_tests"]
                / max(self.overall_results["total_tests"], 1),
            },
            "test_suites": self.test_suites,
            "quality_gates": quality_gate_results,
            "recommendations": self.generate_recommendations(),
        }

        # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        report_file = (
            self.base_path
            / "reports"
            / f"integration_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        with open(report_file, "w") as f:
            json.dump(comprehensive_report, f, indent=2)

        # Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        markdown_report = self.generate_markdown_report(comprehensive_report)
        markdown_file = (
            self.base_path
            / "reports"
            / f"integration_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        )
        with open(markdown_file, "w") as f:
            f.write(markdown_report)

        print(f"ğŸ“„ JSON Report: {report_file}")
        print(f"ğŸ“ Markdown Report: {markdown_file}")

        return comprehensive_report

    def generate_recommendations(self) -> List[str]:
        """æ”¹å–„ææ¡ˆç”Ÿæˆ"""
        recommendations = []

        success_rate = self.overall_results["passed_tests"] / max(
            self.overall_results["total_tests"], 1
        )

        if success_rate < 0.9:
            recommendations.append(
                "ãƒ†ã‚¹ãƒˆæˆåŠŸç‡ãŒ90%ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™ã€‚å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®è©³ç´°èª¿æŸ»ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
            )

        if self.overall_results["error_tests"] > 0:
            recommendations.append(
                "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ†ã‚¹ãƒˆãŒã‚ã‚Šã¾ã™ã€‚ç’°å¢ƒè¨­å®šã‚„ä¾å­˜é–¢ä¿‚ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            )

        if (
            "playwright_e2e" not in self.test_suites
            or self.test_suites["playwright_e2e"].get("status") == "SKIPPED"
        ):
            recommendations.append(
                "E2Eãƒ†ã‚¹ãƒˆãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Playwrightã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            )

        if success_rate >= 0.95:
            recommendations.append("å„ªç§€ãªçµæœã§ã™ï¼ã“ã®å“è³ªã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚")

        return recommendations

    def generate_markdown_report(self, report_data: Dict[str, Any]) -> str:
        """Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        md = "# GitHub Actionsè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ\n\n"

        # ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼
        summary = report_data["executive_summary"]
        md += "## ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼\n\n"
        md += f"**å…¨ä½“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: {summary['overall_status']}\n"
        md += f"**å®Ÿè¡Œæ—¥æ™‚**: {report_data['report_metadata']['generated_at']}\n"
        md += f"**å®Ÿè¡Œæ™‚é–“**: {report_data['report_metadata']['duration_seconds']:.1f}ç§’\n\n"

        md += "### ãƒ†ã‚¹ãƒˆçµæœæ¦‚è¦\n\n"
        md += f"- ç·ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆæ•°: {summary['total_test_suites']}\n"
        md += f"- ç·ãƒ†ã‚¹ãƒˆæ•°: {summary['total_tests']}\n"
        md += f"- æˆåŠŸ: {summary['passed_tests']}\n"
        md += f"- å¤±æ•—: {summary['failed_tests']}\n"
        md += f"- ã‚¨ãƒ©ãƒ¼: {summary['error_tests']}\n"
        md += f"- æˆåŠŸç‡: {summary['success_rate']:.1%}\n\n"

        # å“è³ªã‚²ãƒ¼ãƒˆçµæœ
        md += "## å“è³ªã‚²ãƒ¼ãƒˆçµæœ\n\n"
        for check in report_data["quality_gates"]["checks"]:
            status_emoji = (
                "âœ…"
                if check["status"] == "PASS"
                else "âš ï¸" if check["status"] == "WARN" else "âŒ"
            )
            md += f"{status_emoji} **{check['gate']}**: {check['actual']}\n"
        md += "\n"

        # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆè©³ç´°
        md += "## ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆè©³ç´°\n\n"
        for suite_name, suite_data in report_data["test_suites"].items():
            md += f"### {suite_data.get('test_suite', suite_name)}\n\n"
            md += f"**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: {suite_data.get('status', 'N/A')}\n"

            if "total_tests" in suite_data:
                md += f"**ãƒ†ã‚¹ãƒˆæ•°**: {suite_data['total_tests']}\n"
                md += f"**æˆåŠŸç‡**: {suite_data.get('success_rate', 0):.1%}\n"

            if "message" in suite_data:
                md += f"**ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**: {suite_data['message']}\n"

            md += "\n"

        # æ”¹å–„ææ¡ˆ
        if report_data["recommendations"]:
            md += "## æ”¹å–„ææ¡ˆ\n\n"
            for i, rec in enumerate(report_data["recommendations"], 1):
                md += f"{i}. {rec}\n"
            md += "\n"

        md += "---\n"
        md += "*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ*\n"

        return md

    async def run_full_integration_test_suite(self) -> Dict[str, Any]:
        """å®Œå…¨çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ"""
        print("ğŸš€ Starting Full Integration Test Suite")
        print("ğŸ¯ GitHub Actionsè‡ªå‹•åŒ–ã‚¨ãƒ©ãƒ¼å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ ")
        print("=" * 80)

        try:
            # ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            await self.setup_test_environment()

            # å„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’é †ç•ªã«å®Ÿè¡Œ
            await self.run_github_actions_test_suite()
            await self.run_api_health_checks()
            await self.run_load_performance_tests()
            await self.run_pytest_integration_suite()
            await self.run_playwright_e2e_suite()

            # åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            final_report = await self.generate_comprehensive_report()

            return final_report

        except Exception as e:
            print(f"ğŸ’¥ Integration test suite failed: {e}")
            return {
                "status": "CRITICAL_ERROR",
                "message": str(e),
                "timestamp": datetime.now().isoformat(),
            }


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    runner = IntegrationTestRunner()

    try:
        final_report = await runner.run_full_integration_test_suite()

        # æœ€çµ‚åˆ¤å®š
        overall_status = final_report.get("quality_gates", {}).get(
            "overall_status", "UNKNOWN"
        )

        if overall_status == "PASS":
            print("\nğŸ‰ å…¨ã¦ã®å“è³ªã‚²ãƒ¼ãƒˆã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸï¼")
            print("âœ… GitHub Actionsè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ã¯æœ¬ç•ªç’°å¢ƒã«ãƒ‡ãƒ—ãƒ­ã‚¤å¯èƒ½ã§ã™ã€‚")
            exit_code = 0
        elif overall_status == "WARN":
            print("\nâš ï¸ ä¸€éƒ¨ã®å“è³ªã‚²ãƒ¼ãƒˆã§è­¦å‘ŠãŒã‚ã‚Šã¾ã™ã€‚")
            print("ğŸ” è©³ç´°ãªèª¿æŸ»ã‚’æ¨å¥¨ã—ã¾ã™ãŒã€ãƒ‡ãƒ—ãƒ­ã‚¤ã¯å¯èƒ½ã§ã™ã€‚")
            exit_code = 1
        else:
            print("\nâŒ å“è³ªã‚²ãƒ¼ãƒˆã§ã®å¤±æ•—ãŒã‚ã‚Šã¾ã™ã€‚")
            print("ğŸ› ï¸ ä¿®æ­£ãŒå¿…è¦ã§ã™ã€‚æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ã¯æ¨å¥¨ã•ã‚Œã¾ã›ã‚“ã€‚")
            exit_code = 2

        return exit_code

    except Exception as e:
        print(f"\nğŸ’¥ çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return 3


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)
