name: ITSM CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.12'

jobs:
  # Job 1: Lint and Code Quality
  code-quality:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: 'frontend/package-lock.json'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install frontend dependencies
        working-directory: ./frontend
        run: npm ci

      - name: Install Python dependencies
        working-directory: ./backend
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install flake8 black bandit

      - name: Frontend lint
        working-directory: ./frontend
        run: npm run lint || true

      - name: Python lint with flake8
        working-directory: ./backend
        run: flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics || true

      - name: Security scan with Bandit
        working-directory: ./backend
        run: bandit -r . -f json -o bandit-report.json || true

      - name: Upload code quality artifacts
        uses: actions/upload-artifact@v3
        with:
          name: code-quality-reports
          path: |
            backend/bandit-report.json
          retention-days: 30

  # Job 2: Unit Tests
  unit-tests:
    runs-on: ubuntu-latest
    needs: code-quality
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: 'frontend/package-lock.json'

      - name: Install Python dependencies
        working-directory: ./backend
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-xdist pytest-html

      - name: Install frontend dependencies
        working-directory: ./frontend
        run: npm ci

      - name: Run Python unit tests
        working-directory: ./backend
        run: |
          python -m pytest tests/ -v \
            --tb=short \
            --html=../tests/reports/backend-unit-report.html \
            --self-contained-html \
            --cov=app \
            --cov-report=html:../tests/reports/backend-coverage-html \
            --cov-report=json:../tests/reports/backend-coverage.json \
            --cov-report=term-missing \
            -n 4 || exit 1

      - name: Run frontend unit tests
        working-directory: ./frontend
        run: |
          npm test -- --coverage --watchAll=false --testResultsProcessor=jest-junit || exit 1

      - name: Upload unit test reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: unit-test-reports
          path: |
            tests/reports/backend-unit-report.html
            tests/reports/backend-coverage-html/
            tests/reports/backend-coverage.json
            frontend/coverage/
          retention-days: 30

  # Job 3: API Integration Tests
  api-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: testpassword
          POSTGRES_USER: testuser
          POSTGRES_DB: itsm_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        working-directory: ./backend
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-html pytest-json-report pytest-benchmark

      - name: Setup test database
        working-directory: ./backend
        env:
          DATABASE_URL: postgresql://testuser:testpassword@localhost:5432/itsm_test
        run: |
          python -c "
          from app.db.init_db import init_db
          init_db()
          "

      - name: Run API tests
        working-directory: ./backend
        env:
          DATABASE_URL: postgresql://testuser:testpassword@localhost:5432/itsm_test
          TESTING: true
        run: |
          python -m pytest ../tests/api/ -v \
            --tb=short \
            --html=../tests/reports/api-report.html \
            --self-contained-html \
            --json-report \
            --json-report-file=../tests/reports/api-report.json \
            --benchmark-json=../tests/reports/api-benchmark.json \
            -n 4 || exit 1

      - name: Upload API test reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: api-test-reports
          path: |
            tests/reports/api-report.html
            tests/reports/api-report.json
            tests/reports/api-benchmark.json
          retention-days: 30

  # Job 4: E2E Tests
  e2e-tests:
    runs-on: ubuntu-latest
    needs: api-tests
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: 'frontend/package-lock.json'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install frontend dependencies
        working-directory: ./frontend
        run: npm ci

      - name: Install Playwright browsers
        working-directory: ./frontend
        run: npx playwright install --with-deps

      - name: Install Python dependencies
        working-directory: ./backend
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Build frontend
        working-directory: ./frontend
        run: npm run build

      - name: Start backend server
        working-directory: ./backend
        run: |
          python init_sqlite_db.py
          python run.py &
          sleep 10
        env:
          DATABASE_URL: sqlite:///./test_database.db

      - name: Start frontend server
        working-directory: ./frontend
        run: |
          npm run preview &
          sleep 10

      - name: Wait for servers
        run: |
          timeout 60s bash -c 'until curl -f http://localhost:3000; do sleep 2; done'
          timeout 60s bash -c 'until curl -f http://localhost:8000/health; do sleep 2; done'

      - name: Run E2E tests
        working-directory: ./frontend
        run: |
          npx playwright test \
            --config=../playwright.config.ts \
            --reporter=html,json,junit \
            --output-dir=../tests/reports/e2e-results || exit 1

      - name: Upload E2E test reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: e2e-test-reports
          path: |
            tests/reports/e2e-results/
            tests/reports/playwright-report/
            tests/reports/playwright-results.json
            tests/reports/playwright-results.xml
          retention-days: 30

  # Job 5: Performance Tests
  performance-tests:
    runs-on: ubuntu-latest
    needs: e2e-tests
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        working-directory: ./backend
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-benchmark locust

      - name: Start backend server
        working-directory: ./backend
        run: |
          python init_sqlite_db.py
          python run.py &
          sleep 10
        env:
          DATABASE_URL: sqlite:///./perf_test_database.db

      - name: Run load tests
        working-directory: .
        run: |
          python -m pytest tests/load/ -v \
            --tb=short \
            --html=tests/reports/load-report.html \
            --self-contained-html \
            --benchmark-json=tests/reports/load-benchmark.json \
            --timeout=300 || exit 1

      - name: Upload performance test reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-test-reports
          path: |
            tests/reports/load-report.html
            tests/reports/load-benchmark.json
          retention-days: 30

  # Job 6: Quality Gate Check
  quality-gate:
    runs-on: ubuntu-latest
    needs: [unit-tests, api-tests, e2e-tests, performance-tests]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download all test reports
        uses: actions/download-artifact@v3

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest jinja2

      - name: Generate consolidated report
        run: |
          python tests/utils/generate_consolidated_report.py
          
      - name: Check quality gate
        run: |
          python -c "
          import json
          import sys
          
          try:
              with open('tests/reports/consolidated/summary.json', 'r') as f:
                  data = json.load(f)
              
              success_rate = data['test_execution_summary']['quality_gate_result']['success_rate']
              threshold = data['test_execution_summary']['quality_gate_result']['threshold']
              
              print(f'Test Success Rate: {success_rate}%')
              print(f'Quality Gate Threshold: {threshold}%')
              
              if success_rate >= threshold:
                  print('✅ Quality Gate: PASSED')
                  sys.exit(0)
              else:
                  print('❌ Quality Gate: FAILED')
                  sys.exit(1)
          
          except Exception as e:
              print(f'❌ Quality Gate Check Failed: {e}')
              sys.exit(1)
          "

      - name: Upload consolidated reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: consolidated-reports
          path: |
            tests/reports/consolidated/
          retention-days: 90

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            try {
              const summary = JSON.parse(fs.readFileSync('tests/reports/consolidated/summary.json', 'utf8'));
              const qaResult = summary.test_execution_summary.quality_gate_result;
              
              const comment = `## 🧪 Test Results
              
              **Quality Gate:** ${qaResult.passed ? '✅ PASSED' : '❌ FAILED'}
              **Success Rate:** ${qaResult.success_rate}% (threshold: ${qaResult.threshold}%)
              
              ### Test Summary
              - **Total Tests:** ${summary.test_execution_summary.metrics.total_tests}
              - **Passed:** ${summary.test_execution_summary.metrics.passed_tests}
              - **Failed:** ${summary.test_execution_summary.metrics.failed_tests}
              - **Duration:** ${summary.test_execution_summary.metrics.total_duration}s
              
              [View detailed report in artifacts]
              `;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              console.log('Could not read test results:', error);
            }

  # Job 7: Auto-fix on failure
  auto-fix:
    runs-on: ubuntu-latest
    needs: quality-gate
    if: failure()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download test reports
        uses: actions/download-artifact@v3

      - name: Auto-fix common issues
        run: |
          echo "🔧 Attempting automated fixes..."
          
          # Fix pytest configuration
          cat > pytest.ini << 'EOF'
          [tool:pytest]
          testpaths = tests backend/tests
          python_files = test_*.py *_test.py
          python_classes = Test*
          python_functions = test_*
          addopts = 
              -v 
              --tb=short
              --strict-markers
              --disable-warnings
          markers =
              api: API tests
              unit: Unit tests
              e2e: End-to-end tests
              integration: Integration tests
              slow: Slow running tests
              auth: Authentication tests
              incidents: Incident management tests
              critical: Critical functionality tests
              benchmark: Performance benchmark tests
          EOF
          
          # Create missing unit test structure
          mkdir -p backend/tests/unit
          mkdir -p frontend/src/__tests__
          
          echo "✅ Applied common fixes"

      - name: Commit fixes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action Auto-fix"
          git add .
          git diff --staged --quiet || git commit -m "🤖 Auto-fix: Apply common test configuration fixes
          
          - Updated pytest.ini with proper markers
          - Created missing test directories
          - Fixed test structure for better CI compatibility
          
          Co-Authored-By: GitHub Actions <action@github.com>"

      - name: Push changes
        uses: ad-m/github-push-action@master
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          branch: ${{ github.ref }}

  # Job 8: Notification
  notify:
    runs-on: ubuntu-latest
    needs: [quality-gate, auto-fix]
    if: always()
    steps:
      - name: Notify on success
        if: needs.quality-gate.result == 'success'
        run: |
          echo "✅ All tests passed! Quality gate satisfied."
          
      - name: Notify on failure
        if: needs.quality-gate.result == 'failure'
        run: |
          echo "❌ Tests failed. Auto-fix attempted."
          echo "Please review the test results and fix remaining issues."