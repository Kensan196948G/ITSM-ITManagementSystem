name: ITSM Test Automation

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run nightly tests at 2 AM JST
    - cron: '0 17 * * *'

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      should-run-e2e: ${{ steps.check-changes.outputs.e2e }}
      should-run-load: ${{ steps.check-changes.outputs.load }}
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 2
    
    - name: Check for relevant changes
      id: check-changes
      run: |
        if git diff --name-only HEAD^ HEAD | grep -E '\.(py|js|ts|html|css)$' > /dev/null; then
          echo "e2e=true" >> $GITHUB_OUTPUT
        else
          echo "e2e=false" >> $GITHUB_OUTPUT
        fi
        
        if git diff --name-only HEAD^ HEAD | grep -E 'tests/load/|tests/api/' > /dev/null || [ "${{ github.event_name }}" = "schedule" ]; then
          echo "load=true" >> $GITHUB_OUTPUT
        else
          echo "load=false" >> $GITHUB_OUTPUT
        fi

  unit-and-api-tests:
    runs-on: ubuntu-latest
    needs: setup
    strategy:
      matrix:
        test-type: [api, unit]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-test.txt
        pip install -r backend/requirements.txt
    
    - name: Create test environment file
      run: |
        cp .env.test .env
        cp .env.test backend/.env.test
        echo "ITSM_TEST_MODE=true" >> .env
        echo "ITSM_TEST_MODE=true" >> backend/.env.test
    
    - name: Run ${{ matrix.test-type }} tests
      run: |
        mkdir -p tests/reports
        case "${{ matrix.test-type }}" in
          api)
            cd backend
            python init_sqlite_db.py
            cd ..
            pytest tests/api/ -v --tb=short \
              --html=tests/reports/api-report.html \
              --json-report --json-report-file=tests/reports/api-report.json \
              --cov=backend/app --cov-report=json:tests/reports/api-coverage.json \
              --benchmark-disable \
              -m "api and not slow" || true
            ;;
          unit)
            pytest backend/tests/unit/ -v --tb=short \
              --html=tests/reports/unit-report.html \
              --json-report --json-report-file=tests/reports/unit-report.json \
              --cov=backend/app --cov-report=json:tests/reports/unit-coverage.json \
              -m "unit" || true
            ;;
        esac
    
    - name: Upload test reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: ${{ matrix.test-type }}-test-reports
        path: |
          tests/reports/${{ matrix.test-type }}-*.html
          tests/reports/${{ matrix.test-type }}-*.json
        retention-days: 30
    
    - name: Comment PR with test results
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = 'tests/reports/${{ matrix.test-type }}-report.json';
          
          if (fs.existsSync(path)) {
            const report = JSON.parse(fs.readFileSync(path, 'utf8'));
            const { summary } = report;
            
            const body = `## ${{ matrix.test-type }} Test Results
            
            - **Total Tests**: ${summary.total}
            - **Passed**: ${summary.passed} ✅
            - **Failed**: ${summary.failed} ❌
            - **Skipped**: ${summary.skipped} ⏭️
            - **Duration**: ${summary.duration}s
            
            ${summary.failed > 0 ? '⚠️ Some tests failed. Please check the detailed report.' : '✅ All tests passed!'}
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });
          }

  e2e-tests:
    runs-on: ubuntu-latest
    needs: [setup, unit-and-api-tests]
    # Always run E2E tests to prevent failures due to conditional logic
    # if: needs.setup.outputs.should-run-e2e == 'true'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-test.txt
        pip install -r backend/requirements.txt
    
    - name: Install frontend dependencies
      run: |
        cd frontend
        npm ci
    
    - name: Install Playwright
      run: |
        python -m playwright install chromium
        sudo apt-get update
        sudo apt-get install -y libasound2t64 libatk-bridge2.0-0 libdrm2 libxkbcommon0 libxcomposite1 libxdamage1 libxrandr2 libgbm1 libxss1 libasound2-dev
        python -m playwright install-deps chromium
    
    - name: Create test environment
      run: |
        cp .env.test .env
        cp .env.test backend/.env.test
        echo "HEADLESS=true" >> .env
        echo "RECORD_VIDEO=true" >> .env
        echo "ITSM_TEST_MODE=true" >> .env
        echo "ITSM_TEST_MODE=true" >> backend/.env.test
    
    - name: Initialize test database
      run: |
        cd backend
        python init_sqlite_db.py
    
    - name: Start backend server
      run: |
        cd backend
        python start_server.py &
        sleep 10
        echo "BACKEND_PID=$!" >> $GITHUB_ENV
    
    - name: Build and start frontend server
      run: |
        cd frontend
        npm run build
        npm run preview &
        sleep 10
        echo "FRONTEND_PID=$!" >> $GITHUB_ENV
    
    - name: Wait for servers to be ready
      run: |
        timeout 60 bash -c 'until curl -s http://localhost:8000/health; do sleep 2; done'
        timeout 60 bash -c 'until curl -s http://localhost:3000; do sleep 2; done'
    
    - name: Run E2E tests
      run: |
        pytest tests/e2e/ -v --tb=short \
          --html=tests/reports/e2e-report.html \
          --json-report --json-report-file=tests/reports/e2e-report.json \
          -m "e2e and not slow" \
          --maxfail=5
    
    - name: Stop servers
      if: always()
      run: |
        if [ -n "$BACKEND_PID" ]; then kill $BACKEND_PID || true; fi
        if [ -n "$FRONTEND_PID" ]; then kill $FRONTEND_PID || true; fi
    
    - name: Upload E2E test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: e2e-test-automation-reports
        path: |
          tests/reports/e2e-*.html
          tests/reports/e2e-*.json
          tests/reports/videos/
          tests/reports/screenshots/
        retention-days: 30

  load-tests:
    runs-on: ubuntu-latest
    needs: [setup, unit-and-api-tests]
    if: needs.setup.outputs.should-run-load == 'true'
    timeout-minutes: 30
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-test.txt
    
    - name: Create test environment
      run: |
        cp .env.test .env
        echo "BENCHMARK_ROUNDS=3" >> .env
        echo "BENCHMARK_TIMEOUT=300" >> .env
    
    - name: Run load tests
      run: |
        pytest tests/load/ -v --tb=short \
          --html=tests/reports/load-report.html \
          --json-report --json-report-file=tests/reports/load-report.json \
          --benchmark-json=tests/reports/load-benchmark.json \
          -m "load" \
          --benchmark-sort=mean
    
    - name: Upload load test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: load-test-reports
        path: |
          tests/reports/load-*.html
          tests/reports/load-*.json
        retention-days: 30

  security-tests:
    runs-on: ubuntu-latest
    needs: unit-and-api-tests
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety
    
    - name: Run Bandit security scan
      run: |
        mkdir -p tests/reports
        bandit -r backend/app/ -f json -o tests/reports/bandit-report.json -ll || true
        bandit -r backend/app/ -f txt -ll || true
    
    - name: Check dependencies for vulnerabilities  
      run: |
        mkdir -p tests/reports
        safety check --json > tests/reports/safety-report.json || true
        safety check
    
    - name: Upload security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports
        path: tests/reports/*-report.json
        retention-days: 30

  test-report-generation:
    runs-on: ubuntu-latest
    needs: [unit-and-api-tests, e2e-tests, load-tests, security-tests]
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install jinja2 matplotlib pandas
    
    - name: Download all test reports
      uses: actions/download-artifact@v4
      with:
        path: artifacts/
    
    - name: Generate consolidated test report
      run: |
        python tests/utils/generate_consolidated_report.py \
          --input-dir artifacts/ \
          --output-dir tests/reports/consolidated/
    
    - name: Upload consolidated report
      uses: actions/upload-artifact@v4
      with:
        name: consolidated-test-report
        path: tests/reports/consolidated/
        retention-days: 60
    
    - name: Publish test results to PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Read consolidated report summary
          const summaryPath = 'tests/reports/consolidated/summary.json';
          if (fs.existsSync(summaryPath)) {
            const summary = JSON.parse(fs.readFileSync(summaryPath, 'utf8'));
            
            const body = `## 🧪 Test Results Summary
            
            | Test Type | Total | Passed | Failed | Skipped | Coverage |
            |-----------|-------|--------|--------|---------|----------|
            | Unit Tests | ${summary.unit?.total || 0} | ${summary.unit?.passed || 0} | ${summary.unit?.failed || 0} | ${summary.unit?.skipped || 0} | ${summary.unit?.coverage || 'N/A'} |
            | API Tests | ${summary.api?.total || 0} | ${summary.api?.passed || 0} | ${summary.api?.failed || 0} | ${summary.api?.skipped || 0} | ${summary.api?.coverage || 'N/A'} |
            | E2E Tests | ${summary.e2e?.total || 0} | ${summary.e2e?.passed || 0} | ${summary.e2e?.failed || 0} | ${summary.e2e?.skipped || 0} | N/A |
            | Load Tests | ${summary.load?.total || 0} | ${summary.load?.passed || 0} | ${summary.load?.failed || 0} | ${summary.load?.skipped || 0} | N/A |
            
            **Overall Status**: ${summary.overall?.status === 'success' ? '✅ All tests passed!' : '❌ Some tests failed'}
            
            **Performance Metrics**:
            - Average API Response Time: ${summary.performance?.avg_response_time || 'N/A'}
            - Load Test Success Rate: ${summary.performance?.load_success_rate || 'N/A'}
            
            [📊 View Detailed Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });
          }

  quality-gate:
    runs-on: ubuntu-latest
    needs: [unit-and-api-tests, e2e-tests, load-tests, security-tests]
    if: always() && github.event_name == 'pull_request'
    
    steps:
    - name: Download test reports
      uses: actions/download-artifact@v4
      with:
        path: artifacts/
    
    - name: Quality Gate Check
      run: |
        # Define quality gates
        MIN_COVERAGE=80
        MAX_FAILED_TESTS=0
        MAX_SECURITY_ISSUES=0
        
        # Initialize counters
        TOTAL_FAILED=0
        TOTAL_COVERAGE=0
        SECURITY_ISSUES=0
        
        # Check API test results
        if [ -f "artifacts/api-test-reports/api-report.json" ]; then
          API_FAILED=$(jq -r '.summary.failed' artifacts/api-test-reports/api-report.json)
          TOTAL_FAILED=$((TOTAL_FAILED + API_FAILED))
        fi
        
        # Check E2E test results
        if [ -f "artifacts/e2e-test-reports/e2e-report.json" ]; then
          E2E_FAILED=$(jq -r '.summary.failed' artifacts/e2e-test-reports/e2e-report.json)
          TOTAL_FAILED=$((TOTAL_FAILED + E2E_FAILED))
        fi
        
        # Check coverage
        if [ -f "artifacts/api-test-reports/api-coverage.json" ]; then
          COVERAGE=$(jq -r '.totals.percent_covered' artifacts/api-test-reports/api-coverage.json)
          TOTAL_COVERAGE=$(echo "$COVERAGE" | cut -d. -f1)
        fi
        
        # Check security issues
        if [ -f "artifacts/security-reports/bandit-report.json" ]; then
          SECURITY_ISSUES=$(jq -r '.results | length' artifacts/security-reports/bandit-report.json)
        fi
        
        echo "Quality Gate Results:"
        echo "- Failed Tests: $TOTAL_FAILED (max: $MAX_FAILED_TESTS)"
        echo "- Coverage: $TOTAL_COVERAGE% (min: $MIN_COVERAGE%)"
        echo "- Security Issues: $SECURITY_ISSUES (max: $MAX_SECURITY_ISSUES)"
        
        # Evaluate quality gates
        if [ "$TOTAL_FAILED" -gt "$MAX_FAILED_TESTS" ]; then
          echo "❌ Quality Gate FAILED: Too many test failures"
          exit 1
        fi
        
        if [ "$TOTAL_COVERAGE" -lt "$MIN_COVERAGE" ]; then
          echo "❌ Quality Gate FAILED: Insufficient test coverage"
          exit 1
        fi
        
        if [ "$SECURITY_ISSUES" -gt "$MAX_SECURITY_ISSUES" ]; then
          echo "❌ Quality Gate FAILED: Security issues detected"
          exit 1
        fi
        
        echo "✅ Quality Gate PASSED: All criteria met"

  notification:
    runs-on: ubuntu-latest
    needs: [quality-gate]
    if: always() && (github.event_name == 'schedule' || failure())
    
    steps:
    - name: Notify on failure
      if: failure()
      uses: actions/github-script@v6
      with:
        script: |
          const title = 'ITSM Test Automation Failed';
          const body = `
          Test automation pipeline failed for:
          - Repository: ${{ github.repository }}
          - Branch: ${{ github.ref }}
          - Commit: ${{ github.sha }}
          - Run: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
          
          Please investigate and fix the issues.
          `;
          
          // Create an issue for failed nightly builds
          if ('${{ github.event_name }}' === 'schedule') {
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: title,
              body: body,
              labels: ['bug', 'test-failure', 'automated']
            });
          }