#!/usr/bin/env python3
"""
Quality Gate Checker for CI/CD Pipeline
å“è³ªã‚²ãƒ¼ãƒˆç®¡ç†ã¨ãƒ†ã‚¹ãƒˆçµæœåˆ†æ
"""

import json
import os
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional


class QualityGateChecker:
    """å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚«ãƒ¼"""

    def __init__(self, reports_dir: str = "tests/reports"):
        self.reports_dir = Path(reports_dir)
        self.quality_criteria = {
            "min_test_success_rate": 95.0,  # 95%ä»¥ä¸Šã®æˆåŠŸç‡
            "max_failed_tests": 0,  # å¤±æ•—ãƒ†ã‚¹ãƒˆã¯0ä»¶
            "min_coverage": 80.0,  # 80%ä»¥ä¸Šã®ã‚«ãƒãƒ¬ãƒƒã‚¸
            "max_security_issues": 0,  # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œã¯0ä»¶
            "max_performance_degradation": 20.0,  # 20%ä»¥ä¸Šã®æ€§èƒ½åŠ£åŒ–ã¯è¨±å¯ã—ãªã„
        }

    def analyze_test_reports(self) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’åˆ†æ"""
        analysis = {
            "timestamp": datetime.now().isoformat(),
            "test_suites": {},
            "overall_metrics": {
                "total_tests": 0,
                "passed_tests": 0,
                "failed_tests": 0,
                "skipped_tests": 0,
                "success_rate": 0.0,
            },
            "quality_gate_results": {},
            "recommendations": [],
        }

        # å„ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’åˆ†æ
        for report_file in self.reports_dir.glob("*-report.json"):
            if report_file.exists():
                try:
                    suite_name = report_file.stem.replace("-report", "")
                    with open(report_file, "r", encoding="utf-8") as f:
                        data = json.load(f)

                    suite_metrics = self._extract_test_metrics(data, suite_name)
                    analysis["test_suites"][suite_name] = suite_metrics

                    # å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«åŠ ç®—
                    analysis["overall_metrics"]["total_tests"] += suite_metrics.get(
                        "total", 0
                    )
                    analysis["overall_metrics"]["passed_tests"] += suite_metrics.get(
                        "passed", 0
                    )
                    analysis["overall_metrics"]["failed_tests"] += suite_metrics.get(
                        "failed", 0
                    )
                    analysis["overall_metrics"]["skipped_tests"] += suite_metrics.get(
                        "skipped", 0
                    )

                except Exception as e:
                    print(f"Error analyzing {report_file}: {e}")

        # æˆåŠŸç‡ã‚’è¨ˆç®—
        total_tests = analysis["overall_metrics"]["total_tests"]
        if total_tests > 0:
            passed_tests = analysis["overall_metrics"]["passed_tests"]
            analysis["overall_metrics"]["success_rate"] = (
                passed_tests / total_tests
            ) * 100

        # å“è³ªã‚²ãƒ¼ãƒˆã‚’è©•ä¾¡
        analysis["quality_gate_results"] = self._evaluate_quality_gates(analysis)

        # æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ
        analysis["recommendations"] = self._generate_recommendations(analysis)

        return analysis

    def _extract_test_metrics(
        self, data: Dict[str, Any], suite_name: str
    ) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º"""
        metrics = {
            "suite_name": suite_name,
            "total": 0,
            "passed": 0,
            "failed": 0,
            "skipped": 0,
            "duration": 0.0,
            "coverage": None,
            "performance_issues": [],
        }

        # pytest-json-reportå½¢å¼ã®å ´åˆ
        if "summary" in data:
            summary = data["summary"]
            metrics.update(
                {
                    "total": summary.get("total", 0),
                    "passed": summary.get("passed", 0),
                    "failed": summary.get("failed", 0),
                    "skipped": summary.get("skipped", 0),
                }
            )

        # æœŸé–“æƒ…å ±
        if "duration" in data:
            metrics["duration"] = data["duration"]

        # ãƒ†ã‚¹ãƒˆè©³ç´°ã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
        if "tests" in data:
            for test in data["tests"]:
                if test.get("outcome") == "passed":
                    metrics["passed"] += 1
                elif test.get("outcome") == "failed":
                    metrics["failed"] += 1
                elif test.get("outcome") == "skipped":
                    metrics["skipped"] += 1
                metrics["total"] += 1

        return metrics

    def _evaluate_quality_gates(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """å“è³ªã‚²ãƒ¼ãƒˆã‚’è©•ä¾¡"""
        results = {
            "overall_status": "PASSED",
            "criteria_results": {},
            "failed_criteria": [],
        }

        overall_metrics = analysis["overall_metrics"]
        success_rate = overall_metrics["success_rate"]
        failed_tests = overall_metrics["failed_tests"]

        # ãƒ†ã‚¹ãƒˆæˆåŠŸç‡ãƒã‚§ãƒƒã‚¯
        success_rate_passed = (
            success_rate >= self.quality_criteria["min_test_success_rate"]
        )
        results["criteria_results"]["test_success_rate"] = {
            "status": "PASSED" if success_rate_passed else "FAILED",
            "actual": success_rate,
            "required": self.quality_criteria["min_test_success_rate"],
        }

        if not success_rate_passed:
            results["failed_criteria"].append("test_success_rate")

        # å¤±æ•—ãƒ†ã‚¹ãƒˆæ•°ãƒã‚§ãƒƒã‚¯
        failed_tests_passed = failed_tests <= self.quality_criteria["max_failed_tests"]
        results["criteria_results"]["max_failed_tests"] = {
            "status": "PASSED" if failed_tests_passed else "FAILED",
            "actual": failed_tests,
            "required": self.quality_criteria["max_failed_tests"],
        }

        if not failed_tests_passed:
            results["failed_criteria"].append("max_failed_tests")

        # å…¨ä½“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ±ºå®š
        if results["failed_criteria"]:
            results["overall_status"] = "FAILED"

        return results

    def _generate_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []

        failed_tests = analysis["overall_metrics"]["failed_tests"]
        success_rate = analysis["overall_metrics"]["success_rate"]

        if failed_tests > 0:
            recommendations.append(
                f"å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆ {failed_tests} ä»¶ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„"
            )

        if success_rate < self.quality_criteria["min_test_success_rate"]:
            recommendations.append(
                f"ãƒ†ã‚¹ãƒˆæˆåŠŸç‡ {success_rate:.1f}% ã‚’ {self.quality_criteria['min_test_success_rate']:.1f}% ä»¥ä¸Šã«æ”¹å–„ã—ã¦ãã ã•ã„"
            )

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œã®æ¤œå‡º
        for suite_name, suite_data in analysis["test_suites"].items():
            if suite_data.get("duration", 0) > 60:  # 60ç§’è¶…é
                recommendations.append(
                    f"{suite_name} ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®å®Ÿè¡Œæ™‚é–“ ({suite_data['duration']:.1f}s) ã‚’çŸ­ç¸®ã—ã¦ãã ã•ã„"
                )

        if not recommendations:
            recommendations.append("å…¨å“è³ªåŸºæº–ã‚’ã‚¯ãƒªã‚¢ã—ã¦ã„ã¾ã™ï¼")

        return recommendations

    def generate_report(self, output_file: Optional[str] = None) -> Dict[str, Any]:
        """å“è³ªã‚²ãƒ¼ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        analysis = self.analyze_test_reports()

        if output_file:
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(analysis, f, indent=2, ensure_ascii=False)

            print(f"å“è³ªã‚²ãƒ¼ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {output_path}")

        return analysis

    def print_summary(self, analysis: Dict[str, Any]):
        """ã‚µãƒãƒªã‚’è¡¨ç¤º"""
        print("=" * 60)
        print("          CI/CD å“è³ªã‚²ãƒ¼ãƒˆåˆ†æçµæœ")
        print("=" * 60)

        overall = analysis["overall_metrics"]
        quality_gate = analysis["quality_gate_results"]

        # å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
        print(f"\nğŸ“Š ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚µãƒãƒª:")
        print(f"  ç·ãƒ†ã‚¹ãƒˆæ•°: {overall['total_tests']}")
        print(f"  æˆåŠŸ: {overall['passed_tests']} âœ…")
        print(f"  å¤±æ•—: {overall['failed_tests']} âŒ")
        print(f"  ã‚¹ã‚­ãƒƒãƒ—: {overall['skipped_tests']} â­ï¸")
        print(f"  æˆåŠŸç‡: {overall['success_rate']:.1f}%")

        # å“è³ªã‚²ãƒ¼ãƒˆçµæœè¡¨ç¤º
        print(f"\nğŸš¦ å“è³ªã‚²ãƒ¼ãƒˆçµæœ: {quality_gate['overall_status']}")

        for criteria, result in quality_gate["criteria_results"].items():
            status_icon = "âœ…" if result["status"] == "PASSED" else "âŒ"
            print(f"  {criteria}: {result['status']} {status_icon}")
            print(f"    å®Ÿéš›å€¤: {result['actual']}, è¦æ±‚å€¤: {result['required']}")

        # æ¨å¥¨äº‹é …è¡¨ç¤º
        print(f"\nğŸ’¡ æ¨å¥¨äº‹é …:")
        for i, rec in enumerate(analysis["recommendations"], 1):
            print(f"  {i}. {rec}")

        # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆåˆ¥è©³ç´°
        print(f"\nğŸ“‹ ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆåˆ¥è©³ç´°:")
        for suite_name, suite_data in analysis["test_suites"].items():
            success_rate = (suite_data["passed"] / max(suite_data["total"], 1)) * 100
            print(
                f"  {suite_name}: {suite_data['passed']}/{suite_data['total']} ({success_rate:.1f}%)"
            )


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    checker = QualityGateChecker()

    # å“è³ªã‚²ãƒ¼ãƒˆåˆ†æå®Ÿè¡Œ
    analysis = checker.generate_report("tests/reports/quality-gate-analysis.json")

    # ã‚µãƒãƒªè¡¨ç¤º
    checker.print_summary(analysis)

    # å“è³ªã‚²ãƒ¼ãƒˆå¤±æ•—æ™‚ã®çµ‚äº†ã‚³ãƒ¼ãƒ‰
    if analysis["quality_gate_results"]["overall_status"] == "FAILED":
        print("\nâŒ å“è³ªã‚²ãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ä¸Šè¨˜ã®æ¨å¥¨äº‹é …ã«å¾“ã£ã¦ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)
    else:
        print("\nâœ… å“è³ªã‚²ãƒ¼ãƒˆã«åˆæ ¼ã—ã¾ã—ãŸï¼")
        sys.exit(0)


if __name__ == "__main__":
    main()
